<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>AI Toastmasters Evaluator</title>
  <link rel="icon" href="data:,">
  <style>
    /* â”€â”€â”€ CSS Custom Properties â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
    :root {
      --color-bg: #1a1a2e;
      --color-surface: #16213e;
      --color-surface-alt: #0f3460;
      --color-text: #e8e8e8;
      --color-text-muted: #a0a0b0;
      --color-primary: #4a90d9;
      --color-primary-hover: #5ba0e9;
      --color-success: #27ae60;
      --color-success-hover: #2ecc71;
      --color-danger: #e74c3c;
      --color-danger-hover: #ff5544;
      --color-warning: #f39c12;
      --color-warning-hover: #f1c40f;
      --color-info: #3498db;
      --color-border: #2a2a4a;
      --color-recording: #e74c3c;
      --color-processing: #f39c12;
      --color-delivering: #3498db;
      --font-main: system-ui, -apple-system, "Segoe UI", Roboto, sans-serif;
      --font-mono: "SF Mono", "Fira Code", "Fira Mono", monospace;
      --radius: 8px;
      --radius-lg: 12px;
      --shadow: 0 2px 8px rgba(0, 0, 0, 0.3);
      --transition: 0.2s ease;
    }

    /* â”€â”€â”€ Reset & Base â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
    *, *::before, *::after {
      box-sizing: border-box;
      margin: 0;
      padding: 0;
    }

    html {
      font-size: 18px;
    }

    body {
      font-family: var(--font-main);
      background: var(--color-bg);
      color: var(--color-text);
      min-height: 100vh;
      line-height: 1.5;
    }

    /* â”€â”€â”€ Layout â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
    .app {
      max-width: 960px;
      margin: 0 auto;
      padding: 1.5rem 1rem;
      display: flex;
      flex-direction: column;
      gap: 1.25rem;
      min-height: 100vh;
    }

    /* â”€â”€â”€ Header â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
    .header {
      display: flex;
      align-items: center;
      justify-content: space-between;
      padding-bottom: 0.75rem;
      border-bottom: 1px solid var(--color-border);
    }

    .header h1 {
      font-size: 1.4rem;
      font-weight: 600;
      letter-spacing: -0.02em;
    }

    .header h1 span {
      color: var(--color-primary);
    }

    /* â”€â”€â”€ Status Bar â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
    .status-bar {
      display: flex;
      align-items: center;
      gap: 0.75rem;
      padding: 0.6rem 1rem;
      background: var(--color-surface);
      border-radius: var(--radius);
      border: 1px solid var(--color-border);
      font-size: 0.95rem;
    }

    .status-indicator {
      width: 12px;
      height: 12px;
      border-radius: 50%;
      background: var(--color-text-muted);
      flex-shrink: 0;
      transition: background var(--transition);
    }

    .status-indicator.idle { background: var(--color-text-muted); }
    .status-indicator.recording {
      background: var(--color-recording);
      animation: pulse 1.2s ease-in-out infinite;
    }
    .status-indicator.processing {
      background: var(--color-success);
    }
    .status-indicator.delivering {
      background: var(--color-delivering);
      animation: pulse 1s ease-in-out infinite;
    }

    @keyframes pulse {
      0%, 100% { opacity: 1; }
      50% { opacity: 0.4; }
    }

    .status-text {
      flex: 1;
      font-weight: 500;
    }

    .elapsed-time {
      font-family: var(--font-mono);
      font-size: 1.3rem;
      font-weight: 700;
      color: var(--color-recording);
      display: none;
    }

    .elapsed-time.visible {
      display: block;
    }

    /* â”€â”€â”€ Audio Level Meter â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
    .audio-level {
      display: none;
      width: 80px;
      height: 10px;
      background: var(--color-border);
      border-radius: 5px;
      overflow: hidden;
      flex-shrink: 0;
    }

    .audio-level.visible {
      display: block;
    }

    .audio-level-bar {
      height: 100%;
      width: 0%;
      background: var(--color-success);
      border-radius: 5px;
      transition: width 0.05s linear, background 0.15s ease;
    }

    .audio-level-bar.hot {
      background: var(--color-danger);
    }

    /* â”€â”€â”€ Controls â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
    .controls {
      display: flex;
      flex-wrap: wrap;
      gap: 0.75rem;
      align-items: center;
    }

    .btn {
      display: inline-flex;
      align-items: center;
      gap: 0.5rem;
      padding: 0.65rem 1.25rem;
      border: none;
      border-radius: var(--radius);
      font-family: var(--font-main);
      font-size: 1rem;
      font-weight: 600;
      cursor: pointer;
      transition: background var(--transition), opacity var(--transition), transform 0.1s ease;
      white-space: nowrap;
    }

    .btn:active:not(:disabled) {
      transform: scale(0.97);
    }

    .btn:disabled {
      opacity: 0.4;
      cursor: not-allowed;
    }

    .btn-primary {
      background: var(--color-primary);
      color: #fff;
    }
    .btn-primary:hover:not(:disabled) {
      background: var(--color-primary-hover);
    }

    .btn-success {
      background: var(--color-success);
      color: #fff;
    }
    .btn-success:hover:not(:disabled) {
      background: var(--color-success-hover);
    }

    .btn-danger {
      background: var(--color-danger);
      color: #fff;
    }
    .btn-danger:hover:not(:disabled) {
      background: var(--color-danger-hover);
    }

    .btn-warning {
      background: var(--color-warning);
      color: #1a1a2e;
    }
    .btn-warning:hover:not(:disabled) {
      background: var(--color-warning-hover);
    }

    .btn-save {
      background: var(--color-surface-alt);
      color: var(--color-text);
      border: 1px solid var(--color-border);
    }
    .btn-save:hover:not(:disabled) {
      background: var(--color-primary);
      border-color: var(--color-primary);
    }

    /* Panic mute is always visible and visually distinct */
    .btn-panic {
      background: var(--color-danger);
      color: #fff;
      font-weight: 700;
      text-transform: uppercase;
      letter-spacing: 0.05em;
      border: 2px solid var(--color-danger-hover);
      margin-left: auto;
    }
    .btn-panic:hover:not(:disabled) {
      background: var(--color-danger-hover);
      border-color: #ff7766;
    }

    .hidden {
      display: none !important;
    }

    /* â”€â”€â”€ Speaking Indicator â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
    .speaking-indicator {
      display: none;
      align-items: center;
      justify-content: center;
      gap: 0.75rem;
      padding: 1rem;
      background: var(--color-surface);
      border: 1px solid var(--color-delivering);
      border-radius: var(--radius);
      font-size: 1.15rem;
      font-weight: 600;
      color: var(--color-delivering);
    }

    .speaking-indicator.visible {
      display: flex;
    }

    .speaking-dots {
      display: flex;
      gap: 4px;
    }

    .speaking-dots span {
      width: 8px;
      height: 8px;
      border-radius: 50%;
      background: var(--color-delivering);
      animation: speakingBounce 1.4s ease-in-out infinite;
    }
    .speaking-dots span:nth-child(2) { animation-delay: 0.2s; }
    .speaking-dots span:nth-child(3) { animation-delay: 0.4s; }

    @keyframes speakingBounce {
      0%, 80%, 100% { transform: translateY(0); opacity: 0.4; }
      40% { transform: translateY(-6px); opacity: 1; }
    }

    /* â”€â”€â”€ Processing Indicator â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
    .processing-indicator {
      display: none;
      align-items: center;
      justify-content: center;
      gap: 0.75rem;
      padding: 1rem;
      background: var(--color-surface);
      border: 1px solid var(--color-success);
      border-radius: var(--radius);
      font-size: 1rem;
      color: var(--color-success);
    }

    .processing-indicator.visible {
      display: flex;
    }

    /* â”€â”€â”€ Content Panels â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
    .panel {
      background: var(--color-surface);
      border: 1px solid var(--color-border);
      border-radius: var(--radius-lg);
      overflow: hidden;
    }

    .panel-header {
      display: flex;
      align-items: center;
      justify-content: space-between;
      padding: 0.6rem 1rem;
      background: rgba(255, 255, 255, 0.03);
      border-bottom: 1px solid var(--color-border);
      font-size: 0.85rem;
      font-weight: 600;
      text-transform: uppercase;
      letter-spacing: 0.06em;
      color: var(--color-text-muted);
    }

    .panel-body {
      padding: 1rem;
      min-height: 120px;
      max-height: 400px;
      overflow-y: auto;
    }

    /* â”€â”€â”€ Transcript Area â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
    #transcript-panel {
      display: none;
    }

    #transcript-panel.visible {
      display: block;
    }

    .transcript-content {
      font-size: 1.05rem;
      line-height: 1.7;
      white-space: pre-wrap;
      word-wrap: break-word;
    }

    .transcript-content .segment {
      margin-bottom: 0.5rem;
    }

    .transcript-content .segment-time {
      color: var(--color-text-muted);
      font-family: var(--font-mono);
      font-size: 0.8rem;
      margin-right: 0.5rem;
    }

    .transcript-content .interim {
      color: var(--color-text-muted);
      font-style: italic;
    }

    .transcript-empty {
      color: var(--color-text-muted);
      font-style: italic;
      text-align: center;
      padding: 2rem 0;
    }

    /* â”€â”€â”€ Evaluation Area â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
    #evaluation-panel {
      display: none;
    }

    #evaluation-panel.visible {
      display: block;
    }

    .evaluation-content {
      font-size: 1.05rem;
      line-height: 1.7;
      white-space: pre-wrap;
      word-wrap: break-word;
    }

    .evaluation-empty {
      color: var(--color-text-muted);
      font-style: italic;
      text-align: center;
      padding: 2rem 0;
    }

    /* â”€â”€â”€ Error Banner â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
    .error-banner {
      display: none;
      padding: 0.75rem 1rem;
      border-radius: var(--radius);
      font-size: 0.95rem;
      line-height: 1.4;
    }

    .error-banner.visible {
      display: flex;
      align-items: flex-start;
      gap: 0.75rem;
    }

    .error-banner.recoverable {
      background: rgba(243, 156, 18, 0.15);
      border: 1px solid var(--color-warning);
      color: var(--color-warning);
    }

    .error-banner.non-recoverable {
      background: rgba(231, 76, 60, 0.15);
      border: 1px solid var(--color-danger);
      color: var(--color-danger);
    }

    .error-banner .error-icon {
      font-size: 1.2rem;
      flex-shrink: 0;
      line-height: 1;
    }

    .error-banner .error-message {
      flex: 1;
    }

    .error-banner .error-dismiss {
      background: none;
      border: none;
      color: inherit;
      cursor: pointer;
      font-size: 1.2rem;
      padding: 0;
      line-height: 1;
      opacity: 0.7;
    }

    .error-banner .error-dismiss:hover {
      opacity: 1;
    }

    /* â”€â”€â”€ Interruption Banner â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
    .interruption-banner {
      display: none;
      padding: 0.6rem 1rem;
      background: rgba(231, 76, 60, 0.1);
      border: 1px solid var(--color-danger);
      border-radius: var(--radius);
      color: var(--color-danger);
      font-size: 0.9rem;
      align-items: center;
      gap: 0.5rem;
    }

    .interruption-banner.visible {
      display: flex;
    }

    /* â”€â”€â”€ Saved Confirmation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
    .saved-confirmation {
      display: none;
      padding: 0.6rem 1rem;
      background: rgba(39, 174, 96, 0.15);
      border: 1px solid var(--color-success);
      border-radius: var(--radius);
      color: var(--color-success);
      font-size: 0.9rem;
      align-items: center;
      gap: 0.5rem;
    }

    .saved-confirmation.visible {
      display: flex;
    }

    /* â”€â”€â”€ Consent Form â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
    .consent-form {
      display: none;
      flex-direction: column;
      gap: 0.75rem;
      padding: 1rem;
      background: var(--color-surface);
      border: 1px solid var(--color-border);
      border-radius: var(--radius-lg);
    }

    .consent-form.visible {
      display: flex;
    }

    .consent-form-title {
      font-size: 0.85rem;
      font-weight: 600;
      text-transform: uppercase;
      letter-spacing: 0.06em;
      color: var(--color-text-muted);
      margin-bottom: 0.25rem;
    }

    .consent-form .form-group {
      display: flex;
      flex-direction: column;
      gap: 0.35rem;
    }

    .consent-form label {
      font-size: 0.9rem;
      color: var(--color-text-muted);
    }

    .consent-form input[type="text"] {
      padding: 0.5rem 0.75rem;
      background: var(--color-bg);
      color: var(--color-text);
      border: 1px solid var(--color-border);
      border-radius: var(--radius);
      font-family: var(--font-main);
      font-size: 0.95rem;
      outline: none;
      transition: border-color var(--transition);
    }

    .consent-form input[type="text"]:focus {
      border-color: var(--color-primary);
    }

    .consent-form .checkbox-group {
      display: flex;
      align-items: flex-start;
      gap: 0.5rem;
    }

    .consent-form .checkbox-group input[type="checkbox"] {
      margin-top: 0.25rem;
      accent-color: var(--color-primary);
      width: 16px;
      height: 16px;
      flex-shrink: 0;
    }

    .consent-form .checkbox-group label {
      font-size: 0.9rem;
      color: var(--color-text);
      line-height: 1.4;
      cursor: pointer;
    }

    /* â”€â”€â”€ Consent Status Display â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
    .consent-status {
      display: none;
      align-items: center;
      gap: 0.5rem;
      padding: 0.4rem 0.75rem;
      font-size: 0.85rem;
      color: var(--color-success);
    }

    .consent-status.visible {
      display: flex;
    }

    /* â”€â”€â”€ Time Limit Control â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
    .time-limit-control {
      display: none;
      align-items: center;
      gap: 0.5rem;
      padding: 0.5rem 0;
    }

    .time-limit-control.visible {
      display: flex;
    }

    .time-limit-control label {
      font-size: 0.85rem;
      color: var(--color-text-muted);
      white-space: nowrap;
    }

    .time-limit-control input[type="number"] {
      width: 80px;
      padding: 0.35rem 0.5rem;
      background: var(--color-bg);
      color: var(--color-text);
      border: 1px solid var(--color-border);
      border-radius: var(--radius);
      font-family: var(--font-mono);
      font-size: 0.9rem;
      outline: none;
      transition: border-color var(--transition);
    }

    .time-limit-control input[type="number"]:focus {
      border-color: var(--color-primary);
    }

    .time-limit-control .time-unit {
      font-size: 0.8rem;
      color: var(--color-text-muted);
    }

    /* â”€â”€â”€ Duration Estimate Display â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
    .duration-estimate {
      display: none;
      align-items: center;
      gap: 0.5rem;
      padding: 0.4rem 0.75rem;
      background: rgba(52, 152, 219, 0.1);
      border: 1px solid var(--color-info);
      border-radius: var(--radius);
      font-size: 0.85rem;
      color: var(--color-info);
    }

    .duration-estimate.visible {
      display: flex;
    }

    /* â”€â”€â”€ Revoke Consent Button â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
    .btn-revoke {
      background: transparent;
      color: var(--color-danger);
      border: 1px solid var(--color-danger);
      font-size: 0.8rem;
      padding: 0.4rem 0.75rem;
      font-weight: 600;
    }
    .btn-revoke:hover:not(:disabled) {
      background: rgba(231, 76, 60, 0.15);
    }

    /* â”€â”€â”€ Data Purged Banner â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
    .purge-banner {
      display: none;
      padding: 0.6rem 1rem;
      border-radius: var(--radius);
      font-size: 0.9rem;
      align-items: center;
      gap: 0.5rem;
    }

    .purge-banner.visible {
      display: flex;
    }

    .purge-banner.opt-out {
      background: rgba(231, 76, 60, 0.15);
      border: 1px solid var(--color-danger);
      color: var(--color-danger);
    }

    .purge-banner.auto-purge {
      background: rgba(52, 152, 219, 0.1);
      border: 1px solid var(--color-info);
      color: var(--color-info);
    }

    /* â”€â”€â”€ VAD Notification Banner â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
    .vad-notification {
      display: none;
      padding: 0.6rem 1rem;
      background: rgba(243, 156, 18, 0.15);
      border: 1px solid var(--color-warning);
      border-radius: var(--radius);
      color: var(--color-warning);
      font-size: 0.95rem;
      align-items: center;
      gap: 0.75rem;
    }

    .vad-notification.visible {
      display: flex;
    }

    .vad-notification .vad-notification-text {
      flex: 1;
      font-weight: 500;
    }

    .vad-notification .vad-notification-actions {
      display: flex;
      gap: 0.5rem;
      flex-shrink: 0;
    }

    .vad-notification .btn-vad-confirm {
      padding: 0.35rem 0.75rem;
      background: var(--color-warning);
      color: #1a1a2e;
      border: none;
      border-radius: var(--radius);
      font-family: var(--font-main);
      font-size: 0.85rem;
      font-weight: 600;
      cursor: pointer;
      transition: background var(--transition);
    }

    .vad-notification .btn-vad-confirm:hover {
      background: var(--color-warning-hover);
    }

    .vad-notification .btn-vad-dismiss {
      padding: 0.35rem 0.75rem;
      background: transparent;
      color: var(--color-warning);
      border: 1px solid var(--color-warning);
      border-radius: var(--radius);
      font-family: var(--font-main);
      font-size: 0.85rem;
      font-weight: 600;
      cursor: pointer;
      transition: background var(--transition);
    }

    .vad-notification .btn-vad-dismiss:hover {
      background: rgba(243, 156, 18, 0.1);
    }

    /* â”€â”€â”€ Project Context Form â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
    .project-context-form {
      display: none;
      flex-direction: column;
      gap: 0.75rem;
      padding: 1rem;
      background: var(--color-surface);
      border: 1px solid var(--color-border);
      border-radius: var(--radius-lg);
    }

    .project-context-form.visible {
      display: flex;
    }

    .project-context-form-title {
      font-size: 0.85rem;
      font-weight: 600;
      text-transform: uppercase;
      letter-spacing: 0.06em;
      color: var(--color-text-muted);
      margin-bottom: 0.25rem;
    }

    .project-context-form .form-group {
      display: flex;
      flex-direction: column;
      gap: 0.35rem;
    }

    .project-context-form label {
      font-size: 0.9rem;
      color: var(--color-text-muted);
    }

    .project-context-form input[type="text"],
    .project-context-form select,
    .project-context-form textarea {
      padding: 0.5rem 0.75rem;
      background: var(--color-bg);
      color: var(--color-text);
      border: 1px solid var(--color-border);
      border-radius: var(--radius);
      font-family: var(--font-main);
      font-size: 0.95rem;
      outline: none;
      transition: border-color var(--transition);
    }

    .project-context-form input[type="text"]:focus,
    .project-context-form select:focus,
    .project-context-form textarea:focus {
      border-color: var(--color-primary);
    }

    .project-context-form select {
      cursor: pointer;
      appearance: auto;
    }

    .project-context-form textarea {
      min-height: 80px;
      resize: vertical;
      line-height: 1.5;
    }

    /* â”€â”€â”€ VAD Config Control â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
    .vad-config {
      display: none;
      flex-direction: column;
      gap: 0.5rem;
      padding: 0.75rem 1rem;
      background: var(--color-surface);
      border: 1px solid var(--color-border);
      border-radius: var(--radius-lg);
    }

    .vad-config.visible {
      display: flex;
    }

    .vad-config-title {
      font-size: 0.85rem;
      font-weight: 600;
      text-transform: uppercase;
      letter-spacing: 0.06em;
      color: var(--color-text-muted);
      margin-bottom: 0.1rem;
    }

    .vad-config-row {
      display: flex;
      align-items: center;
      gap: 0.75rem;
      flex-wrap: wrap;
    }

    .vad-toggle-group {
      display: flex;
      align-items: center;
      gap: 0.5rem;
    }

    .vad-toggle-group label {
      font-size: 0.9rem;
      color: var(--color-text);
      cursor: pointer;
    }

    .vad-toggle-group input[type="checkbox"] {
      accent-color: var(--color-primary);
      width: 16px;
      height: 16px;
      cursor: pointer;
    }

    .vad-slider-group {
      display: flex;
      align-items: center;
      gap: 0.5rem;
      flex: 1;
    }

    .vad-slider-group label {
      font-size: 0.85rem;
      color: var(--color-text-muted);
      white-space: nowrap;
    }

    .vad-slider-group input[type="range"] {
      flex: 1;
      min-width: 100px;
      accent-color: var(--color-primary);
      cursor: pointer;
    }

    .vad-slider-group .vad-value {
      font-family: var(--font-mono);
      font-size: 0.85rem;
      color: var(--color-text);
      min-width: 2.5em;
      text-align: right;
    }

    /* â”€â”€â”€ Video Preview (Phase 4 â€” Req 2.1, 2.2) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
    .video-preview-container {
      display: none;
      align-items: center;
      gap: 0.75rem;
      padding: 0.5rem;
    }

    .video-preview-container.visible {
      display: flex;
    }

    .video-preview-label {
      font-size: 0.8rem;
      color: var(--color-text-muted);
      text-transform: uppercase;
      letter-spacing: 0.04em;
    }

    .video-preview {
      width: 120px;
      height: 90px;
      border-radius: var(--radius);
      border: 1px solid var(--color-border);
      background: #000;
      object-fit: cover;
    }

    .video-frame-stats {
      font-family: var(--font-mono);
      font-size: 0.75rem;
      color: var(--color-text-muted);
    }

    .video-latency-warning {
      color: #e67e22;
      margin-left: 0.25rem;
      cursor: help;
    }

    .video-quality-grade {
      display: none;
      font-family: var(--font-mono);
      font-size: 0.75rem;
      padding: 0.15rem 0.5rem;
      border-radius: var(--radius);
      font-weight: 600;
    }

    .video-quality-grade.visible {
      display: inline-block;
    }

    .video-quality-grade.good {
      background: #d4edda;
      color: #155724;
    }

    .video-quality-grade.degraded {
      background: #fff3cd;
      color: #856404;
    }

    .video-quality-grade.poor {
      background: #f8d7da;
      color: #721c24;
    }

    /* â”€â”€â”€ Video FPS Config (Phase 4 â€” Req 2.9) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
    .video-fps-config {
      display: none;
      flex-direction: column;
      gap: 0.5rem;
      padding: 0.75rem 1rem;
      background: var(--color-surface);
      border: 1px solid var(--color-border);
      border-radius: var(--radius-lg);
    }

    .video-fps-config.visible {
      display: flex;
    }

    .video-fps-config-title {
      font-size: 0.85rem;
      font-weight: 600;
      text-transform: uppercase;
      letter-spacing: 0.06em;
      color: var(--color-text-muted);
      margin-bottom: 0.1rem;
    }

    .video-fps-config-row {
      display: flex;
      align-items: center;
      gap: 0.75rem;
    }

    .video-fps-slider-group {
      display: flex;
      align-items: center;
      gap: 0.5rem;
      flex: 1;
    }

    .video-fps-slider-group label {
      font-size: 0.85rem;
      color: var(--color-text-muted);
      white-space: nowrap;
    }

    .video-fps-slider-group input[type="range"] {
      flex: 1;
      min-width: 100px;
      accent-color: var(--color-primary);
      cursor: pointer;
    }

    .video-fps-slider-group .video-fps-value {
      font-family: var(--font-mono);
      font-size: 0.85rem;
      color: var(--color-text);
      min-width: 3.5em;
      text-align: right;
    }

    /* â”€â”€â”€ Video Consent Error (Phase 4 â€” Req 1.5) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
    .video-consent-error {
      font-size: 0.8rem;
      color: var(--color-danger);
      display: none;
      padding: 0.25rem 0;
    }

    .video-consent-error.visible {
      display: block;
    }

    /* â”€â”€â”€ Evidence Link (Phase 3 â€” Req 7.1) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
    .evidence-link {
      color: var(--color-primary);
      cursor: pointer;
      text-decoration: underline;
      text-decoration-style: dotted;
      text-underline-offset: 2px;
      border-radius: 2px;
      padding: 0 2px;
      transition: background var(--transition), color var(--transition);
    }

    .evidence-link:hover {
      background: rgba(74, 144, 217, 0.15);
      color: var(--color-primary-hover);
      text-decoration-style: solid;
    }

    /* Evidence quotes that couldn't be matched â€” no clickable navigation (Req 7.4) */
    .evidence-no-match {
      color: var(--color-text-muted);
      font-style: italic;
    }

    /* â”€â”€â”€ Segment Highlight (Phase 3 â€” Req 7.2, 7.5) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
    .segment-highlight {
      background: rgba(74, 144, 217, 0.25);
      border-left: 3px solid var(--color-primary);
      padding-left: 0.5rem;
      border-radius: 2px;
      transition: background 0.3s ease, border-color 0.3s ease;
    }

    /* â”€â”€â”€ Footer â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
    .footer {
      margin-top: auto;
      padding-top: 0.75rem;
      border-top: 1px solid var(--color-border);
      font-size: 0.75rem;
      color: var(--color-text-muted);
      text-align: center;
    }
  </style>
</head>
<body>
  <div class="app">
    <!-- Header -->
    <header class="header">
      <h1><span>AI</span> Toastmasters Evaluator</h1>
      <span id="connection-status" style="font-size: 0.8rem; color: var(--color-text-muted);">Disconnected</span>
    </header>

    <!-- Error Banner -->
    <div id="error-banner" class="error-banner" role="alert" aria-live="assertive">
      <span class="error-icon">âš </span>
      <span id="error-message" class="error-message"></span>
      <button class="error-dismiss" onclick="dismissError()" aria-label="Dismiss error">&times;</button>
    </div>

    <!-- Interruption Banner (shown after panic mute) -->
    <div id="interruption-banner" class="interruption-banner" role="status">
      <span>âš¡</span>
      <span>Session interrupted. You can start a new recording or attempt evaluation from captured data.</span>
    </div>

    <!-- Saved Confirmation -->
    <div id="saved-confirmation" class="saved-confirmation" role="status">
      <span>âœ“</span>
      <span id="saved-message">Outputs saved successfully.</span>
    </div>

    <!-- Data Purged Banner -->
    <div id="purge-banner" class="purge-banner" role="alert" aria-live="assertive">
      <span id="purge-icon">ğŸ—‘</span>
      <span id="purge-message"></span>
    </div>

    <!-- Status Bar -->
    <div class="status-bar">
      <div id="status-indicator" class="status-indicator idle"></div>
      <span id="status-text" class="status-text">Ready â€” click "Start Speech" to begin</span>
      <span id="consent-status" class="consent-status" role="status" aria-live="polite"></span>
      <span id="elapsed-time" class="elapsed-time">00:00</span>
      <div id="audio-level" class="audio-level" aria-label="Audio input level">
        <div id="audio-level-bar" class="audio-level-bar"></div>
      </div>
    </div>

    <!-- Consent Form (visible in IDLE state) -->
    <div id="consent-form" class="consent-form" role="form" aria-label="Speaker consent">
      <div class="consent-form-title">Speaker Consent</div>
      <div class="form-group">
        <label for="speaker-name-input">Speaker's name</label>
        <input type="text" id="speaker-name-input" placeholder="Speaker's name" required aria-required="true" autocomplete="off">
      </div>
      <div class="checkbox-group">
        <input type="checkbox" id="consent-checkbox" aria-describedby="consent-label">
        <label id="consent-label" for="consent-checkbox">I confirm the speaker has given verbal consent to be recorded and evaluated</label>
      </div>
      <div class="checkbox-group">
        <input type="checkbox" id="video-consent-checkbox" aria-describedby="video-consent-label" disabled>
        <label id="video-consent-label" for="video-consent-checkbox">Enable video capture for visual delivery feedback</label>
      </div>
      <div id="video-consent-error" class="video-consent-error" role="alert" aria-live="polite"></div>
    </div>

    <!-- Project Context Form (visible in IDLE state) -->
    <div id="project-context-form" class="project-context-form" role="form" aria-label="Project context">
      <div class="project-context-form-title">Project Context</div>
      <div class="form-group">
        <label for="speech-title-input">Speech title</label>
        <input type="text" id="speech-title-input" placeholder="e.g. My Journey to Toastmasters" maxlength="200" autocomplete="off">
      </div>
      <div class="form-group">
        <label for="project-type-select">Project type</label>
        <select id="project-type-select">
          <option value="">â€” Select project type â€”</option>
          <option value="Ice Breaker">Ice Breaker</option>
          <option value="Evaluation and Feedback">Evaluation and Feedback</option>
          <option value="Researching and Presenting">Researching and Presenting</option>
          <option value="Introduction to Vocal Variety">Introduction to Vocal Variety</option>
          <option value="Connect with Storytelling">Connect with Storytelling</option>
          <option value="Persuasive Speaking">Persuasive Speaking</option>
          <option value="Custom / Other">Custom / Other</option>
        </select>
      </div>
      <div class="form-group">
        <label for="objectives-textarea">Project objectives</label>
        <textarea id="objectives-textarea" placeholder="One objective per line (auto-populated on project type selection)" rows="4"></textarea>
      </div>
    </div>

    <!-- Time Limit Control (visible in IDLE and PROCESSING states) -->
    <div id="time-limit-control" class="time-limit-control" aria-label="Evaluation time limit">
      <label for="time-limit-input">Evaluation time limit</label>
      <input type="number" id="time-limit-input" value="120" min="30" max="600" step="10" aria-label="Time limit in seconds">
      <span class="time-unit">seconds</span>
    </div>

    <!-- VAD Configuration (visible in IDLE state) -->
    <div id="vad-config" class="vad-config" aria-label="Voice Activity Detection configuration">
      <div class="vad-config-title">Voice Activity Detection</div>
      <div class="vad-config-row">
        <div class="vad-toggle-group">
          <input type="checkbox" id="vad-enabled-checkbox" checked aria-label="Enable voice activity detection">
          <label for="vad-enabled-checkbox">Enable VAD</label>
        </div>
        <div class="vad-slider-group">
          <label for="vad-threshold-slider">Silence threshold</label>
          <input type="range" id="vad-threshold-slider" min="3" max="15" value="5" step="1" aria-label="Silence threshold in seconds">
          <span id="vad-threshold-value" class="vad-value">5s</span>
        </div>
      </div>
    </div>

    <!-- Video FPS Configuration (visible in IDLE state when video consent enabled â€” Req 2.9) -->
    <div id="video-fps-config" class="video-fps-config" aria-label="Video frame rate configuration">
      <div class="video-fps-config-title">Video Configuration</div>
      <div class="video-fps-config-row">
        <div class="video-fps-slider-group">
          <label for="video-fps-slider">Frame rate</label>
          <input type="range" id="video-fps-slider" min="1" max="5" value="2" step="1" aria-label="Video frame rate in FPS">
          <span id="video-fps-value" class="video-fps-value">2 FPS</span>
        </div>
      </div>
    </div>

    <!-- VAD Notification Banner (shown during RECORDING when speech-end detected) -->
    <div id="vad-notification" class="vad-notification" role="alert" aria-live="assertive">
      <span>ğŸ””</span>
      <span class="vad-notification-text">Speech likely ended â€” confirm stop?</span>
      <div class="vad-notification-actions">
        <button class="btn-vad-confirm" onclick="onVADConfirmStop()">Confirm Stop</button>
        <button class="btn-vad-dismiss" onclick="onVADDismiss()">Dismiss</button>
      </div>
    </div>

    <!-- Duration Estimate (visible in PROCESSING state near Deliver button) -->
    <div id="duration-estimate" class="duration-estimate" role="status" aria-live="polite">
      <span>â±</span>
      <span id="duration-estimate-text">Estimated: --:-- / Limit: 2:00</span>
    </div>

    <!-- Video Preview (Phase 4 â€” Req 2.1, 2.2) -->
    <div id="video-preview-container" class="video-preview-container" aria-label="Camera preview">
      <span class="video-preview-label">Camera</span>
      <video id="video-preview" class="video-preview" autoplay muted playsinline></video>
      <canvas id="video-capture-canvas" style="display:none;"></canvas>
      <span id="video-frame-stats" class="video-frame-stats"></span>
    </div>

    <!-- Controls -->
    <div class="controls">
      <button id="btn-start" class="btn btn-success" onclick="onStartSpeech()">
        â— Start Speech
      </button>
      <button id="btn-stop" class="btn btn-primary hidden" onclick="onStopSpeech()">
        â–  Stop Speech
      </button>
      <button id="btn-deliver" class="btn btn-warning hidden" onclick="onDeliverEvaluation()">
        â–¶ Deliver Evaluation
      </button>
      <button id="btn-replay" class="btn btn-primary hidden" onclick="onReplayEvaluation()">
        ğŸ” Replay Evaluation
      </button>
      <button id="btn-save" class="btn btn-save hidden" onclick="onSaveOutputs()">
        ğŸ’¾ Save Outputs
      </button>
      <button id="btn-revoke" class="btn btn-revoke hidden" onclick="onRevokeConsent()" aria-label="Revoke speaker consent and purge data">
        âœ• Revoke Consent
      </button>
      <button id="btn-panic" class="btn btn-panic" onclick="onPanicMute()">
        ğŸ”‡ Panic Mute
      </button>
    </div>

    <!-- Speaking Indicator -->
    <div id="speaking-indicator" class="speaking-indicator" role="status" aria-live="polite">
      <div class="speaking-dots">
        <span></span><span></span><span></span>
      </div>
      Speaking...
    </div>

    <!-- Processing Indicator -->
    <div id="processing-indicator" class="processing-indicator" role="status" aria-live="polite">
      <span>Processing speech...</span>
    </div>

    <!-- Transcript Panel -->
    <section id="transcript-panel" class="panel" aria-label="Live Transcript">
      <div class="panel-header">
        <span>Live Transcript</span>
        <span id="transcript-word-count"></span>
      </div>
      <div class="panel-body">
        <div id="transcript-content" class="transcript-content">
          <div class="transcript-empty">Transcript will appear here during recording...</div>
        </div>
      </div>
    </section>

    <!-- Evaluation Panel -->
    <section id="evaluation-panel" class="panel" aria-label="Evaluation">
      <div class="panel-header">
        <span>Evaluation</span>
        <span id="video-quality-grade" class="video-quality-grade" aria-live="polite"></span>
      </div>
      <div class="panel-body">
        <div id="evaluation-content" class="evaluation-content">
          <div class="evaluation-empty">Evaluation will appear here after delivery...</div>
        </div>
      </div>
    </section>

    <!-- Footer -->
    <footer class="footer">
      AI Toastmasters Evaluator &mdash; Phase 2
    </footer>
  </div>

  <script>
    // â”€â”€â”€ Session State Constants â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    const SessionState = Object.freeze({
      IDLE: "idle",
      RECORDING: "recording",
      PROCESSING: "processing",
      DELIVERING: "delivering",
    });

    // â”€â”€â”€ Application State â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    let currentState = SessionState.IDLE;
    let hasEvaluationData = false;
    let hasTTSAudio = false;
    let outputsSaved = false;
    let segments = []; // local transcript segment array

    // â”€â”€â”€ Phase 2: Consent & Time Limit State â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    let consentConfirmed = false;
    let consentSpeakerName = "";
    let dataPurged = false;
    let estimatedDuration = null;
    let configuredTimeLimit = 120;

    // â”€â”€â”€ Eager Pipeline State â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    /** @type {string} Current pipeline stage for UI display and button gating */
    let pipelineStage = "idle";
    /** @type {number} RunId for stale-progress filtering; 0 = accept all */
    let pipelineRunId = 0;

    // â”€â”€â”€ Phase 3: VAD Configuration State â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    /** Whether VAD is enabled (default: true, Req 3.5) */
    let vadEnabled = true;
    /** Silence threshold in seconds (default: 5, range 3-15, Req 3.1) */
    let vadSilenceThreshold = 5;
    /** Whether the VAD notification banner is currently visible (Req 2.2) */
    let vadNotificationVisible = false;

    // â”€â”€â”€ Phase 3: Project Context State (Req 4.1, 4.2, 4.3, 4.4) â”€â”€â”€â”€
    /** Client-side project context state, synced to server via set_project_context */
    let projectContext = { speechTitle: "", projectType: "", objectives: [] };

    // â”€â”€â”€ Phase 4: Video Consent State (Req 1.1, 1.2, 1.5, 2.9) â”€â”€â”€â”€â”€â”€
    /** Whether video consent toggle is checked */
    let videoConsentEnabled = false;
    /** Whether camera was successfully acquired after video consent */
    let videoStreamReady = false;
    /** Configured video FPS (1-5, default 2) */
    let videoFpsConfig = 2;

    /** Predefined Toastmasters Pathways project types and their standard objectives (Req 4.2) */
    const PROJECT_TYPES = {
      "Ice Breaker": [
        "Introduce yourself and share your personal story",
        "Organize your speech with an opening, body, and conclusion",
        "Speak for 4-6 minutes"
      ],
      "Evaluation and Feedback": [
        "Present a speech on any topic",
        "Receive and apply feedback from evaluators",
        "Speak for 5-7 minutes"
      ],
      "Researching and Presenting": [
        "Research a topic and present your findings",
        "Use credible sources to support your points",
        "Speak for 5-7 minutes"
      ],
      "Introduction to Vocal Variety": [
        "Use vocal variety to enhance your message",
        "Vary pace, pitch, volume, and pauses",
        "Speak for 5-7 minutes"
      ],
      "Connect with Storytelling": [
        "Share a personal story that connects with the audience",
        "Use vivid language and emotional appeal",
        "Speak for 5-7 minutes"
      ],
      "Persuasive Speaking": [
        "Persuade the audience to adopt your viewpoint",
        "Use logical arguments and emotional appeals",
        "Speak for 5-7 minutes"
      ],
      "Custom / Other": []
    };

    // â”€â”€â”€ Phase 3: VAD Audio Level Meter State (Req 10.1, 10.3, 10.4) â”€â”€
    /** Timestamp (ms) of the last received vad_status message */
    let lastVadStatusTime = 0;
    /** Whether to use server-side VAD energy for the audio level meter.
     *  True when vadEnabled AND we've received a vad_status recently (within 2s). */
    let useVadEnergy = false;

    // â”€â”€â”€ TTS Audio Playback State â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    /** @type {HTMLAudioElement|null} Audio element for TTS playback */
    let ttsAudioElement = null;
    /** @type {string|null} Current Blob URL for TTS audio */
    let ttsBlobUrl = null;
    /** Whether TTS playback is currently active */
    let ttsPlaying = false;
    /** Whether the server has signaled TTS delivery is complete */
    let ttsDeliveryComplete = false;
    /** The last evaluation script text, used as fallback on TTS error */
    let lastEvaluationScript = "";
    /** @type {Object|null} The last StructuredEvaluationPublic object for evidence linking (Phase 3, Req 7.1) */
    let lastEvaluationData = null;
    /** @type {number|null} Timer ID for auto-dismissing segment highlights (Phase 3, Req 7.5) */
    let highlightDismissTimer = null;
    /** Monotonic counter for playback instance identification.
     *  Incremented by handleTTSAudio (new playback) and cancelDeferredIdle (force-stop).
     *  No other code path may increment it â€” the latch's +1 progression check depends on this. */
    let playbackInstanceToken = 0;
    /** Pending deferred IDLE transition, or null if none.
     *  Shape: { token: number } | null */
    let deferredIdleTransition = null;
    /**
     * Token-stamped latch for the "IDLE arrives before ttsPlaying=true" ordering edge case.
     * Shape: { tokenAtLatch: number } | null
     *
     * Set when state_change: idle arrives while in any non-IDLE state but ttsPlaying is false
     * (uses currentState !== IDLE). Stores the current playbackInstanceToken at latch time.
     * Idempotent: won't re-latch if already set. Does NOT fire on redundant IDLEâ†’IDLE.
     *
     * Consumed by handleTTSAudio with token validation: only creates deferral if
     * latchToken + 1 === currentToken (expected progression); otherwise discards as stale â€”
     * prevents a latent latch from attaching to unrelated audio.
     *
     * Cleared by: handleTTSAudio (consumed or discarded), forceStopTtsAndCancelDeferral()
     * (force-stop paths), and transitionToIdle() (normal IDLE application).
     */
    let pendingIdleFromServer = null;

    // â”€â”€â”€ Audio Capture State â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    /** @type {WebSocket|null} */
    let ws = null;
    /** @type {AudioContext|null} */
    let audioContext = null;
    /** @type {AudioWorkletNode|null} */
    let workletNode = null;
    /** @type {MediaStream|null} */
    let mediaStream = null;
    /** @type {MediaStreamAudioSourceNode|null} */
    let sourceNode = null;
    /** Whether the audio_format handshake has been sent for this connection */
    let audioFormatSent = false;
    /** Whether we are in the post-TTS cooldown period */
    let inCooldown = false;
    /** Cooldown timer ID */
    let cooldownTimer = null;
    /** Cooldown duration in ms (2.5 seconds â€” midpoint of 2-3 second range) */
    const COOLDOWN_MS = 2500;

    // â”€â”€â”€ DOM References â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    const dom = {
      statusIndicator: document.getElementById("status-indicator"),
      statusText: document.getElementById("status-text"),
      elapsedTime: document.getElementById("elapsed-time"),
      btnStart: document.getElementById("btn-start"),
      btnStop: document.getElementById("btn-stop"),
      btnDeliver: document.getElementById("btn-deliver"),
      btnReplay: document.getElementById("btn-replay"),
      btnSave: document.getElementById("btn-save"),
      btnPanic: document.getElementById("btn-panic"),
      btnRevoke: document.getElementById("btn-revoke"),
      speakingIndicator: document.getElementById("speaking-indicator"),
      processingIndicator: document.getElementById("processing-indicator"),
      transcriptPanel: document.getElementById("transcript-panel"),
      transcriptContent: document.getElementById("transcript-content"),
      transcriptWordCount: document.getElementById("transcript-word-count"),
      evaluationPanel: document.getElementById("evaluation-panel"),
      evaluationContent: document.getElementById("evaluation-content"),
      errorBanner: document.getElementById("error-banner"),
      errorMessage: document.getElementById("error-message"),
      interruptionBanner: document.getElementById("interruption-banner"),
      savedConfirmation: document.getElementById("saved-confirmation"),
      savedMessage: document.getElementById("saved-message"),
      connectionStatus: document.getElementById("connection-status"),
      audioLevel: document.getElementById("audio-level"),
      audioLevelBar: document.getElementById("audio-level-bar"),
      // Phase 2: Consent & Time Limit DOM refs
      consentForm: document.getElementById("consent-form"),
      speakerNameInput: document.getElementById("speaker-name-input"),
      consentCheckbox: document.getElementById("consent-checkbox"),
      consentStatus: document.getElementById("consent-status"),
      timeLimitControl: document.getElementById("time-limit-control"),
      timeLimitInput: document.getElementById("time-limit-input"),
      durationEstimate: document.getElementById("duration-estimate"),
      durationEstimateText: document.getElementById("duration-estimate-text"),
      purgeBanner: document.getElementById("purge-banner"),
      purgeMessage: document.getElementById("purge-message"),
      // Phase 3: VAD Config DOM refs
      vadConfig: document.getElementById("vad-config"),
      vadEnabledCheckbox: document.getElementById("vad-enabled-checkbox"),
      vadThresholdSlider: document.getElementById("vad-threshold-slider"),
      vadThresholdValue: document.getElementById("vad-threshold-value"),
      // Phase 3: VAD Notification DOM ref
      vadNotification: document.getElementById("vad-notification"),
      // Phase 3: Project Context DOM refs
      projectContextForm: document.getElementById("project-context-form"),
      speechTitleInput: document.getElementById("speech-title-input"),
      projectTypeSelect: document.getElementById("project-type-select"),
      objectivesTextarea: document.getElementById("objectives-textarea"),
      // Phase 4: Video Consent DOM refs
      videoConsentCheckbox: document.getElementById("video-consent-checkbox"),
      videoConsentError: document.getElementById("video-consent-error"),
      videoFpsConfig: document.getElementById("video-fps-config"),
      videoFpsSlider: document.getElementById("video-fps-slider"),
      videoFpsValue: document.getElementById("video-fps-value"),
    };

    // â”€â”€â”€ Status Text Map â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    const STATUS_TEXT = {
      [SessionState.IDLE]: "Ready â€” click \"Start Speech\" to begin",
      [SessionState.RECORDING]: "Recording speech...",
      [SessionState.PROCESSING]: "Speech processed â€” click \"Deliver Evaluation\" when ready",
      [SessionState.DELIVERING]: "Delivering evaluation...",
    };

    // â”€â”€â”€ UI Update: Main State Machine â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    /**
     * Updates the entire UI based on the current session state.
     * Shows/hides buttons, indicators, and panels according to the
     * state machine defined in the design document.
     *
     * @param {string} state - One of SessionState values
     */
    function updateUI(state) {
      currentState = state;

      // Update status indicator
      dom.statusIndicator.className = "status-indicator " + state;
      dom.statusText.textContent = STATUS_TEXT[state] || "Unknown state";

      // Hide all transient banners on state change (except errors and purge)
      hide(dom.interruptionBanner);
      hide(dom.savedConfirmation);

      // Dismiss VAD notification on any state change (Req 2.5, 2.6)
      dismissVADNotification();

      // Reset VAD energy state when leaving RECORDING (Req 10.4)
      // Safe to call on every state change â€” no-op when already reset.
      resetVadEnergyState();

      // Update consent status display throughout all states
      updateConsentStatusDisplay();

      // â”€â”€ IDLE â”€â”€
      if (state === SessionState.IDLE) {
        // Show consent form only in IDLE (and not after purge â€” form is reset but still shown)
        show(dom.consentForm);
        // Show project context form in IDLE (Req 4.1)
        show(dom.projectContextForm);
        // Show time limit control in IDLE
        show(dom.timeLimitControl);
        // Show VAD config in IDLE (Req 3.1, 3.4)
        show(dom.vadConfig);
        // Show video FPS config in IDLE only when video consent is enabled (Req 2.9)
        if (videoConsentEnabled) {
          show(dom.videoFpsConfig);
        } else {
          hide(dom.videoFpsConfig);
        }
        // Show video preview in IDLE only when video consent is enabled and camera is active
        if (videoConsentEnabled && videoStream) {
          show(videoDom.previewContainer);
        } else {
          hide(videoDom.previewContainer);
        }
        // Enable video consent checkbox in IDLE
        enable(dom.videoConsentCheckbox);
        // Hide duration estimate in IDLE
        hide(dom.durationEstimate);

        // Start Speech gated on consent
        show(dom.btnStart);
        if (consentConfirmed && consentSpeakerName.trim().length > 0 && !inCooldown) {
          enable(dom.btnStart);
        } else {
          disable(dom.btnStart);
        }

        hide(dom.btnStop);
        hide(dom.btnDeliver);
        // Show Save Outputs only if evaluation data exists, not already saved, and not purged
        if (hasEvaluationData && !outputsSaved && !dataPurged) {
          show(dom.btnSave);
        } else {
          hide(dom.btnSave);
        }
        // Show Replay button if TTS audio was received and evaluation data exists and not purged
        if (hasTTSAudio && hasEvaluationData && !dataPurged) {
          show(dom.btnReplay);
        } else {
          hide(dom.btnReplay);
        }
        // Disable replay during cooldown
        if (inCooldown) {
          disable(dom.btnReplay);
        } else {
          enable(dom.btnReplay);
        }
        enable(dom.btnPanic);

        // Show Revoke Consent button when consent is confirmed (allows opt-out)
        if (consentConfirmed) {
          show(dom.btnRevoke);
        } else {
          hide(dom.btnRevoke);
        }

        hide(dom.speakingIndicator);
        hide(dom.processingIndicator);
        hideElapsedTime();
        hide(dom.audioLevel);

        return;
      }

      // â”€â”€ RECORDING â”€â”€
      if (state === SessionState.RECORDING) {
        // Hide consent form during recording (consent is immutable)
        hide(dom.consentForm);
        // Hide project context form during recording (Req 4.7 â€” immutable after recording starts)
        hide(dom.projectContextForm);
        // Hide time limit control during recording
        hide(dom.timeLimitControl);
        // Hide VAD config during recording (Req 3.4)
        hide(dom.vadConfig);
        // Hide video FPS config during recording (Req 1.4 â€” immutable after recording starts)
        hide(dom.videoFpsConfig);
        // Disable video consent checkbox during recording (Req 1.4)
        disable(dom.videoConsentCheckbox);
        // Keep video preview visible during recording so operator can verify camera is working
        hide(dom.durationEstimate);

        hide(dom.btnStart);
        show(dom.btnStop);
        hide(dom.btnDeliver);
        hide(dom.btnSave);
        hide(dom.btnReplay);
        enable(dom.btnStop);
        enable(dom.btnPanic);

        // Show Revoke Consent during recording (opt-out is always available)
        if (consentConfirmed) {
          show(dom.btnRevoke);
        } else {
          hide(dom.btnRevoke);
        }

        hide(dom.speakingIndicator);
        hide(dom.processingIndicator);
        showElapsedTime();
        show(dom.audioLevel);

        // Show transcript panel for live captions
        show(dom.transcriptPanel);

        return;
      }

      // â”€â”€ PROCESSING â”€â”€
      if (state === SessionState.PROCESSING) {
        // Hide consent form during processing
        hide(dom.consentForm);
        // Hide project context form during processing (Req 4.7 â€” immutable after recording starts)
        hide(dom.projectContextForm);
        // Show time limit control in PROCESSING (can still adjust before delivery)
        show(dom.timeLimitControl);
        // Hide VAD config during processing (Req 3.4)
        hide(dom.vadConfig);
        // Hide video FPS config during processing
        hide(dom.videoFpsConfig);
        // Disable video consent checkbox during processing
        disable(dom.videoConsentCheckbox);
        // Hide video preview during processing
        hide(videoDom.previewContainer);
        // Show duration estimate if available
        if (estimatedDuration !== null) {
          show(dom.durationEstimate);
        }

        hide(dom.btnStart);
        hide(dom.btnStop);
        show(dom.btnDeliver);
        hide(dom.btnSave);
        hide(dom.btnReplay);
        // Deliver button gating based on pipeline stage (Req 4.1-4.4)
        // Disable during in-progress stages and initial idle; enable only on terminal/actionable stages
        // Server remains authoritative â€” UI gating is advisory only (Req 4.4)
        if (pipelineStage === "ready" || pipelineStage === "failed" || pipelineStage === "invalidated") {
          enable(dom.btnDeliver);
        } else {
          disable(dom.btnDeliver);
        }
        enable(dom.btnPanic);

        // Show Revoke Consent during processing
        if (consentConfirmed) {
          show(dom.btnRevoke);
        } else {
          hide(dom.btnRevoke);
        }

        hide(dom.speakingIndicator);
        show(dom.processingIndicator);
        // Update processing indicator text to reflect current pipeline stage
        updateProcessingIndicator(pipelineStage);
        hideElapsedTime();
        hide(dom.audioLevel);

        // Keep transcript visible
        show(dom.transcriptPanel);

        return;
      }

      // â”€â”€ DELIVERING â”€â”€
      if (state === SessionState.DELIVERING) {
        // Hide consent form during delivery
        hide(dom.consentForm);
        // Hide project context form during delivery (Req 4.7 â€” immutable after recording starts)
        hide(dom.projectContextForm);
        // Hide time limit and duration estimate during delivery
        hide(dom.timeLimitControl);
        // Hide VAD config during delivery (Req 3.4)
        hide(dom.vadConfig);
        // Hide video FPS config during delivery
        hide(dom.videoFpsConfig);
        // Disable video consent checkbox during delivery
        disable(dom.videoConsentCheckbox);
        // Hide video preview during delivery
        hide(videoDom.previewContainer);
        hide(dom.durationEstimate);

        hide(dom.btnStart);
        hide(dom.btnStop);
        hide(dom.btnDeliver);
        hide(dom.btnSave);
        hide(dom.btnReplay);
        hide(dom.btnRevoke);
        // Disable all actions except Panic Mute during delivery
        enable(dom.btnPanic);

        show(dom.speakingIndicator);
        hide(dom.processingIndicator);
        hideElapsedTime();
        hide(dom.audioLevel);

        // Show evaluation panel for fallback reading
        show(dom.evaluationPanel);
        // Keep transcript visible
        show(dom.transcriptPanel);

        return;
      }
    }

    // â”€â”€â”€ UI Update: Audio Level Meter â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    /**
     * Updates the audio level indicator bar.
     * Prefers server-side VAD energy when available (Req 10.3).
     * Falls back to client-side AudioWorklet RMS when VAD energy is
     * unavailable or stale (Req 10.4).
     *
     * When called with a source of "vad", the energy value comes from
     * the server's vad_status message (already 0..1 normalized).
     * When called with a source of "worklet" (or no source), the rms
     * value comes from the AudioWorklet (0..1 range, typical speech
     * 0.01-0.15).
     *
     * @param {number} rms - RMS audio level (0..1)
     * @param {"worklet"|"vad"} [source="worklet"] - Source of the level data
     */
    function updateAudioLevel(rms, source) {
      // If VAD energy is active and this is a worklet update, skip it (Req 10.3).
      // The server-side VAD energy takes priority when available.
      if (source !== "vad" && useVadEnergy) {
        return;
      }

      // Scale: multiply by ~5 and clamp to 100% for visual responsiveness
      const pct = Math.min(100, rms * 500);
      dom.audioLevelBar.style.width = pct + "%";
      // Turn red when clipping (RMS > 0.3 is very loud)
      if (rms > 0.3) {
        dom.audioLevelBar.classList.add("hot");
      } else {
        dom.audioLevelBar.classList.remove("hot");
      }
    }

    /**
     * Handles vad_status messages from the server.
     * Updates the audio level meter with server-side VAD energy (Req 10.1, 10.3).
     * Tracks the last receive time for the 2-second fallback (Req 10.4).
     * @param {Object} message - The vad_status message with energy and isSpeech fields
     */
    function handleVADStatus(message) {
      // Only process during RECORDING state
      if (currentState !== SessionState.RECORDING) return;

      // When VAD is disabled, ignore vad_status messages â€” use AudioWorklet path
      if (!vadEnabled) return;

      // Track the last time we received a vad_status for fallback logic
      lastVadStatusTime = Date.now();
      useVadEnergy = true;

      // Drive the audio level meter with server-side energy
      updateAudioLevel(message.energy, "vad");
    }

    /**
     * Checks whether the VAD energy source has gone stale (no vad_status
     * received for 2+ seconds) and falls back to AudioWorklet if so (Req 10.4).
     * Called from the AudioWorklet onmessage handler during RECORDING.
     */
    function checkVadEnergyFallback() {
      if (!useVadEnergy) return;

      const now = Date.now();
      if (now - lastVadStatusTime >= 2000) {
        // VAD energy has gone stale â€” fall back to AudioWorklet
        useVadEnergy = false;
      }
    }

    /**
     * Resets VAD audio level meter state. Called when leaving RECORDING state
     * or when VAD is disabled.
     */
    function resetVadEnergyState() {
      lastVadStatusTime = 0;
      useVadEnergy = false;
    }

    // â”€â”€â”€ UI Update: Elapsed Time â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    /**
     * Updates the elapsed time display during recording.
     * @param {number} seconds - Elapsed seconds since recording started
     */
    function updateElapsedTime(seconds) {
      const mins = Math.floor(seconds / 60);
      const secs = seconds % 60;
      const formatted = String(mins).padStart(2, "0") + ":" + String(secs).padStart(2, "0");
      dom.elapsedTime.textContent = formatted;
    }

    // â”€â”€â”€ UI Update: Consent Status Display â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    /**
     * Updates the consent status display in the status bar.
     * Shows "Speaker: Name âœ“ Consent confirmed" when consent is set.
     */
    function updateConsentStatusDisplay() {
      if (consentConfirmed && consentSpeakerName.trim().length > 0) {
        dom.consentStatus.textContent = "Speaker: " + consentSpeakerName + " \u2713 Consent confirmed";
        show(dom.consentStatus);
      } else {
        dom.consentStatus.textContent = "";
        hide(dom.consentStatus);
      }
    }

    // â”€â”€â”€ UI Update: Duration Estimate â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    /**
     * Updates the duration estimate display.
     * @param {number} estimatedSeconds - Estimated evaluation duration in seconds
     * @param {number} timeLimitSeconds - Configured time limit in seconds
     */
    function updateDurationEstimateDisplay(estimatedSeconds, timeLimitSeconds) {
      const estMins = Math.floor(estimatedSeconds / 60);
      const estSecs = Math.floor(estimatedSeconds % 60);
      const limMins = Math.floor(timeLimitSeconds / 60);
      const limSecs = Math.floor(timeLimitSeconds % 60);
      const estStr = String(estMins) + ":" + String(estSecs).padStart(2, "0");
      const limStr = String(limMins) + ":" + String(limSecs).padStart(2, "0");
      dom.durationEstimateText.textContent = "Estimated: " + estStr + " / Limit: " + limStr;
    }

    // â”€â”€â”€ Consent Form Event Handlers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    // â”€â”€â”€ UI Update: Processing Indicator (Eager Pipeline) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    /**
     * Updates the processing indicator text based on the current pipeline stage.
     * Maps PipelineStage values to user-facing status messages.
     * @param {string} stage - One of the PipelineStage values
     */
    function updateProcessingIndicator(stage) {
      const messages = {
        processing_speech: "Speech processed \u2014 preparing evaluation...",
        generating_evaluation: "Generating evaluation...",
        synthesizing_audio: "Synthesizing audio...",
        ready: "\u2713 Evaluation ready \u2014 click \"Deliver Evaluation\"",
        failed: "\u26A0 Evaluation generation failed \u2014 click \"Deliver Evaluation\" to retry",
        invalidated: "Settings changed \u2014 evaluation will regenerate on delivery",
      };
      const span = dom.processingIndicator.querySelector("span");
      if (span) {
        span.textContent = messages[stage] || "Processing...";
      }
    }

    /**
     * Updates the Deliver button enabled/disabled state based on pipeline stage.
     * Disable during in-progress stages; enable on terminal/actionable stages.
     * Server remains authoritative â€” UI gating is advisory only (Req 4.4).
     * @param {string} stage - One of the PipelineStage values
     */
    function updateDeliverButtonState(stage) {
      if (currentState !== SessionState.PROCESSING) return;
      if (stage === "ready" || stage === "failed" || stage === "invalidated") {
        enable(dom.btnDeliver);
      } else {
        disable(dom.btnDeliver);
      }
    }

    // â”€â”€â”€ Consent Form Event Handlers (continued) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    /**
     * Called when the speaker name input or consent checkbox changes.
     * Sends set_consent message to server and updates local state.
     */
    function onConsentChange() {
      const name = dom.speakerNameInput.value.trim();
      const checked = dom.consentCheckbox.checked;

      consentSpeakerName = name;
      consentConfirmed = checked && name.length > 0;

      // Send consent state to server
      wsSend({
        type: "set_consent",
        speakerName: name,
        consentConfirmed: checked && name.length > 0,
      });

      // Enable/disable video consent checkbox based on audio consent (Req 1.1)
      if (consentConfirmed) {
        enable(dom.videoConsentCheckbox);
      } else {
        // If audio consent is revoked, also disable and uncheck video consent
        disable(dom.videoConsentCheckbox);
        if (videoConsentEnabled) {
          dom.videoConsentCheckbox.checked = false;
          videoConsentEnabled = false;
          videoStreamReady = false;
          releaseCamera();
          hideVideoConsentError();
        }
      }

      // Update Start Speech button gating
      updateUI(currentState);
    }

    /**
     * Called when the video consent checkbox changes.
     * Acquires camera on enable, releases on disable.
     * Sends set_video_consent and video_stream_ready messages. (Req 1.1, 1.2, 1.5)
     */
    async function onVideoConsentChange() {
      const checked = dom.videoConsentCheckbox.checked;
      hideVideoConsentError();

      if (checked) {
        // Attempt to acquire camera (Req 1.5)
        const acquired = await acquireCamera();
        if (acquired) {
          videoConsentEnabled = true;
          videoStreamReady = true;

          // Send video consent to server (Req 1.2)
          wsSend({
            type: "set_video_consent",
            consentGranted: true,
            timestamp: new Date().toISOString(),
          });

          // Send video_stream_ready to server (Req 1.5)
          var readyMsg = {
            type: "video_stream_ready",
          };
          // Include device label only if available (optional per Req 10.2)
          if (videoStream) {
            var tracks = videoStream.getVideoTracks();
            if (tracks.length > 0 && tracks[0].label) {
              readyMsg.deviceLabel = tracks[0].label;
            }
          }
          wsSend(readyMsg);
        } else {
          // Camera acquisition failed â€” revert toggle (Req 1.5)
          dom.videoConsentCheckbox.checked = false;
          videoConsentEnabled = false;
          videoStreamReady = false;
          showVideoConsentError("Camera access denied or unavailable. Video consent requires camera permission.");
        }
      } else {
        // Video consent disabled â€” release camera
        videoConsentEnabled = false;
        videoStreamReady = false;
        releaseCamera();
      }

      // Update UI to show/hide video preview and FPS config
      updateUI(currentState);
    }

    /**
     * Shows a video consent error message below the checkbox.
     * @param {string} message - Error message to display
     */
    function showVideoConsentError(message) {
      dom.videoConsentError.textContent = message;
      dom.videoConsentError.classList.add("visible");
    }

    /**
     * Hides the video consent error message.
     */
    function hideVideoConsentError() {
      dom.videoConsentError.textContent = "";
      dom.videoConsentError.classList.remove("visible");
    }

    /**
     * Called when the video FPS slider changes.
     * Sends set_video_config message to server. (Req 2.9)
     */
    function onVideoFpsChange() {
      var fps = parseInt(dom.videoFpsSlider.value, 10);
      if (isNaN(fps) || fps < 1 || fps > 5) return;

      videoFpsConfig = fps;
      dom.videoFpsValue.textContent = fps + " FPS";

      // Send video config to server (Req 2.9)
      wsSend({
        type: "set_video_config",
        frameRate: fps,
      });
    }

    /**
     * Updates the FPS slider display value on input (live feedback).
     */
    function onVideoFpsInput() {
      dom.videoFpsValue.textContent = dom.videoFpsSlider.value + " FPS";
    }

    /**
     * Called when the time limit input changes.
     * Sends set_time_limit message to server.
     */
    function onTimeLimitChange() {
      const seconds = parseInt(dom.timeLimitInput.value, 10);
      if (isNaN(seconds) || seconds < 30) return;

      configuredTimeLimit = seconds;

      // Send time limit to server
      wsSend({
        type: "set_time_limit",
        seconds: seconds,
      });

      // Update duration estimate display if we have an estimate
      if (estimatedDuration !== null) {
        updateDurationEstimateDisplay(estimatedDuration, configuredTimeLimit);
      }
    }

    // â”€â”€â”€ Phase 3: VAD Notification Handlers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    /**
     * Handles vad_speech_end message from the server.
     * Shows the VAD notification banner during RECORDING state.
     * If a second vad_speech_end arrives while banner is visible,
     * replaces the existing banner (resets state) rather than showing
     * a second one (Req 2.8).
     * @param {Object} message - The vad_speech_end message
     */
    function handleVADSpeechEnd(message) {
      // Only show notification during RECORDING state
      if (currentState !== SessionState.RECORDING) return;

      // If banner is already visible, reset its state (Req 2.8)
      // This is effectively a no-op visually since the banner text is the same,
      // but it resets any internal state associated with the notification.
      vadNotificationVisible = true;
      show(dom.vadNotification);
    }

    /**
     * Handles "Confirm Stop" button click on the VAD notification banner.
     * Sends stop_recording and dismisses the banner (Req 2.3).
     */
    function onVADConfirmStop() {
      dismissVADNotification();
      // Follow the existing stop recording flow (Req 2.3)
      onStopSpeech();
    }

    /**
     * Handles "Dismiss" button click on the VAD notification banner.
     * Hides the banner and continues recording (Req 2.4).
     */
    function onVADDismiss() {
      dismissVADNotification();
    }

    /**
     * Dismisses the VAD notification banner and resets state.
     * Called on Dismiss click, Confirm Stop click, state change away
     * from RECORDING, and manual Stop Speech click (Req 2.4, 2.5, 2.6).
     */
    function dismissVADNotification() {
      if (vadNotificationVisible) {
        vadNotificationVisible = false;
        hide(dom.vadNotification);
      }
    }

    // â”€â”€â”€ Phase 3: VAD Config Event Handlers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    /**
     * Called when the VAD enabled checkbox or silence threshold slider changes.
     * Updates local state and sends set_vad_config message to server.
     * (Req 3.1, 3.2, 3.5)
     */
    function onVADConfigChange() {
      vadEnabled = dom.vadEnabledCheckbox.checked;
      vadSilenceThreshold = parseInt(dom.vadThresholdSlider.value, 10);

      // When VAD is disabled, immediately fall back to AudioWorklet (Req 10.4 note)
      if (!vadEnabled) {
        resetVadEnergyState();
      }

      // Update the displayed value label
      dom.vadThresholdValue.textContent = vadSilenceThreshold + "s";

      // Send VAD config to server
      wsSend({
        type: "set_vad_config",
        silenceThresholdSeconds: vadSilenceThreshold,
        enabled: vadEnabled,
      });
    }

    /**
     * Called on slider input (live update of displayed value while dragging).
     * Does NOT send a WebSocket message â€” that happens on change.
     */
    function onVADThresholdInput() {
      dom.vadThresholdValue.textContent = dom.vadThresholdSlider.value + "s";
    }

    // â”€â”€â”€ Phase 3: Project Context Event Handlers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    /**
     * Called when the speech title input changes.
     * Updates local state and sends set_project_context message to server.
     * (Req 4.1, 4.4)
     */
    function onSpeechTitleChange() {
      projectContext.speechTitle = dom.speechTitleInput.value.trim();
      sendProjectContext();
    }

    /**
     * Called when the project type dropdown changes.
     * Auto-populates objectives for predefined project types (Req 4.3).
     * Updates local state and sends set_project_context message to server.
     * (Req 4.2, 4.3, 4.4)
     */
    function onProjectTypeChange() {
      const selectedType = dom.projectTypeSelect.value;
      projectContext.projectType = selectedType;

      // Auto-populate objectives for predefined project types (Req 4.3)
      if (selectedType && PROJECT_TYPES[selectedType] && PROJECT_TYPES[selectedType].length > 0) {
        const objectives = PROJECT_TYPES[selectedType];
        dom.objectivesTextarea.value = objectives.join("\n");
        projectContext.objectives = objectives.slice();
      } else if (selectedType === "") {
        // Cleared selection â€” clear objectives
        dom.objectivesTextarea.value = "";
        projectContext.objectives = [];
      }
      // For "Custom / Other" (empty array), leave objectives as-is so operator can type freely

      sendProjectContext();
    }

    /**
     * Called when the objectives textarea changes.
     * Parses one objective per line, updates local state, and sends to server.
     * (Req 4.1, 4.4)
     */
    function onObjectivesChange() {
      const text = dom.objectivesTextarea.value;
      // Parse objectives: one per line, filter out empty lines
      projectContext.objectives = text.split("\n")
        .map(function(line) { return line.trim(); })
        .filter(function(line) { return line.length > 0; });
      sendProjectContext();
    }

    /**
     * Sends the current project context to the server via WebSocket.
     * (Req 4.4)
     */
    function sendProjectContext() {
      wsSend({
        type: "set_project_context",
        speechTitle: projectContext.speechTitle,
        projectType: projectContext.projectType,
        objectives: projectContext.objectives,
      });
    }

    /**
     * Resets the project context form to its default empty state.
     * Called on data_purged and other reset scenarios.
     */
    function resetProjectContextForm() {
      projectContext = { speechTitle: "", projectType: "", objectives: [] };
      dom.speechTitleInput.value = "";
      dom.projectTypeSelect.value = "";
      dom.objectivesTextarea.value = "";
    }

    // â”€â”€â”€ UI Update: Transcript â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    /**
     * Updates the transcript display using replaceFromIndex splice semantics.
     * The client maintains a local segment array and splices from
     * replaceFromIndex onward with the new segments.
     *
     * @param {Array} newSegments - Replacement suffix segments
     * @param {number} replaceFromIndex - Index to splice from
     */
    function updateTranscript(newSegments, replaceFromIndex) {
      // Splice local segment array
      segments.splice(replaceFromIndex, segments.length - replaceFromIndex, ...newSegments);

      // Render segments
      renderTranscript();
    }

    /**
     * Renders the current segments array into the transcript panel.
     */
    function renderTranscript() {
      if (segments.length === 0) {
        dom.transcriptContent.innerHTML =
          '<div class="transcript-empty">Transcript will appear here during recording...</div>';
        dom.transcriptWordCount.textContent = "";
        return;
      }

      let html = "";
      let totalWords = 0;

      for (let idx = 0; idx < segments.length; idx++) {
        const seg = segments[idx];
        const timeStr = formatTimestamp(seg.startTime);
        const cssClass = seg.isFinal ? "" : " interim";
        const words = seg.text.trim().split(/\s+/).filter(Boolean);
        totalWords += words.length;

        html += '<div class="segment' + cssClass + '" data-segment-index="' + idx + '">';
        html += '<span class="segment-time">[' + timeStr + ']</span>';
        html += escapeHtml(seg.text);
        html += "</div>";
      }

      dom.transcriptContent.innerHTML = html;
      dom.transcriptWordCount.textContent = totalWords + " words";

      // Auto-scroll to bottom
      const panelBody = dom.transcriptContent.parentElement;
      panelBody.scrollTop = panelBody.scrollHeight;
    }

    // â”€â”€â”€ Phase 3: Evidence Highlight Functions (Req 7.1-7.5) â”€â”€â”€â”€â”€â”€â”€â”€

    /**
     * Normalizes text for evidence quote matching.
     * Matches the server-side EvidenceValidator.normalize() logic:
     *  1. Lowercase
     *  2. Strip all non-alphanumeric non-whitespace characters
     *  3. Collapse consecutive whitespace to a single space
     *  4. Trim leading/trailing whitespace
     *
     * (Req 7.3)
     * @param {string} text - The text to normalize
     * @returns {string} Normalized text
     */
    function normalizeForMatch(text) {
      return text.toLowerCase().replace(/[^a-z0-9\s]/g, "").replace(/\s+/g, " ").trim();
    }

    /**
     * Finds the transcript segment that contains the given evidence quote.
     * Uses the same normalization rules as the server-side EvidenceValidator (Req 7.3).
     *
     * @param {string} quote - The evidence quote to find
     * @param {Array} segs - The transcript segments array
     * @returns {{segmentIndex: number, segment: Object}|null} Match result or null
     */
    function findTranscriptMatch(quote, segs) {
      const normalizedQuote = normalizeForMatch(quote);
      if (normalizedQuote.length === 0) return null;

      for (let i = 0; i < segs.length; i++) {
        const segText = normalizeForMatch(segs[i].text);
        if (segText.includes(normalizedQuote)) {
          return { segmentIndex: i, segment: segs[i] };
        }
      }
      return null;
    }

    /**
     * Handles clicking on an evidence link in the evaluation panel.
     * Scrolls the transcript panel to the matching segment and highlights it.
     * Auto-dismisses the highlight after 3 seconds (Req 7.5).
     *
     * @param {string} quote - The evidence quote text
     */
    function onEvidenceLinkClick(quote) {
      // Clear any existing highlight and timer
      clearEvidenceHighlight();

      // Find the matching segment
      var match = findTranscriptMatch(quote, segments);
      if (!match) return;

      // Find the segment DOM element by data-segment-index
      var segEl = dom.transcriptContent.querySelector(
        '[data-segment-index="' + match.segmentIndex + '"]'
      );
      if (!segEl) return;

      // Add highlight class
      segEl.classList.add("segment-highlight");

      // Scroll the transcript panel to the highlighted segment
      var panelBody = dom.transcriptContent.parentElement;
      segEl.scrollIntoView({ behavior: "smooth", block: "center" });

      // Make sure the transcript panel is visible
      show(dom.transcriptPanel);

      // Auto-dismiss highlight after 3 seconds (Req 7.5)
      highlightDismissTimer = setTimeout(function () {
        clearEvidenceHighlight();
      }, 3000);
    }

    /**
     * Clears any active evidence highlight from the transcript panel.
     * Cancels the auto-dismiss timer if active (Req 7.5).
     */
    function clearEvidenceHighlight() {
      // Clear the timer
      if (highlightDismissTimer !== null) {
        clearTimeout(highlightDismissTimer);
        highlightDismissTimer = null;
      }

      // Remove highlight class from all segments
      var highlighted = dom.transcriptContent.querySelectorAll(".segment-highlight");
      for (var i = 0; i < highlighted.length; i++) {
        highlighted[i].classList.remove("segment-highlight");
      }
    }

    /**
     * Renders the evaluation text with evidence quotes as clickable links.
     * Evidence quotes from evaluation items are matched against the text
     * and wrapped in clickable <span> elements (Req 7.1).
     *
     * Redacted quotes (containing "[a fellow member]") will not match
     * transcript text and are displayed without clickable navigation (Req 7.4).
     *
     * @param {string} text - The evaluation script text
     * @param {Object|null} evaluationData - The StructuredEvaluationPublic object
     */
    function renderEvaluationWithEvidence(text, evaluationData) {
      if (!text || text.trim().length === 0) {
        dom.evaluationContent.innerHTML =
          '<div class="evaluation-empty">Evaluation will appear here after delivery...</div>';
        return;
      }

      // If no evaluation data with items, fall back to plain text rendering
      if (!evaluationData || !evaluationData.items || evaluationData.items.length === 0) {
        dom.evaluationContent.innerHTML = '<div>' + escapeHtml(text) + '</div>';
        return;
      }

      // Build a list of evidence quotes with their match status
      var evidenceQuotes = [];
      for (var i = 0; i < evaluationData.items.length; i++) {
        var item = evaluationData.items[i];
        if (item.evidence_quote && item.evidence_quote.trim().length > 0) {
          var match = findTranscriptMatch(item.evidence_quote, segments);
          evidenceQuotes.push({
            quote: item.evidence_quote,
            timestamp: item.evidence_timestamp,
            hasMatch: match !== null
          });
        }
      }

      // Sort evidence quotes by length descending to avoid partial replacement issues
      // (longer quotes should be replaced first)
      evidenceQuotes.sort(function (a, b) {
        return b.quote.length - a.quote.length;
      });

      // Escape the full text first, then replace evidence quotes with clickable spans
      var escapedText = escapeHtml(text);

      // Track which portions of the text have already been replaced to avoid
      // double-replacement when one quote is a substring of another
      var replacedRanges = [];

      for (var q = 0; q < evidenceQuotes.length; q++) {
        var ev = evidenceQuotes[q];
        var escapedQuote = escapeHtml(ev.quote);

        // Find the quote in the escaped text, respecting already-replaced ranges
        var searchStart = 0;
        var foundIndex = -1;

        while (searchStart < escapedText.length) {
          var idx = escapedText.indexOf(escapedQuote, searchStart);
          if (idx === -1) break;

          // Check if this range overlaps with any already-replaced range
          var overlaps = false;
          for (var r = 0; r < replacedRanges.length; r++) {
            if (idx < replacedRanges[r].end && idx + escapedQuote.length > replacedRanges[r].start) {
              overlaps = true;
              break;
            }
          }

          if (!overlaps) {
            foundIndex = idx;
            break;
          }

          searchStart = idx + 1;
        }

        if (foundIndex === -1) continue;

        // Build the replacement span
        var spanHtml;
        if (ev.hasMatch) {
          // Clickable evidence link (Req 7.1)
          var safeQuote = ev.quote.replace(/"/g, "&quot;").replace(/'/g, "&#39;");
          spanHtml = '<span class="evidence-link" data-quote="' + safeQuote +
            '" data-timestamp="' + ev.timestamp +
            '" onclick="onEvidenceLinkClick(this.dataset.quote)" title="Click to navigate to transcript">' +
            escapedQuote + '</span>';
        } else {
          // No match â€” display without clickable navigation (Req 7.4)
          spanHtml = '<span class="evidence-no-match">' + escapedQuote + '</span>';
        }

        // Replace in the escaped text
        var before = escapedText.substring(0, foundIndex);
        var after = escapedText.substring(foundIndex + escapedQuote.length);
        escapedText = before + spanHtml + after;

        // Track the replaced range (using the new span length)
        replacedRanges.push({
          start: foundIndex,
          end: foundIndex + spanHtml.length
        });
      }

      dom.evaluationContent.innerHTML = '<div>' + escapedText + '</div>';
    }

    // â”€â”€â”€ UI Update: Evaluation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    /**
     * Displays the evaluation text in the evaluation panel.
     * Used both for normal display and as TTS fallback.
     * When evaluation data with evidence items is available,
     * renders evidence quotes as clickable links (Phase 3, Req 7.1).
     *
     * @param {string} text - The evaluation script text
     */
    function showEvaluation(text) {
      hasEvaluationData = true;

      if (!text || text.trim().length === 0) {
        dom.evaluationContent.innerHTML =
          '<div class="evaluation-empty">Evaluation will appear here after delivery...</div>';
        return;
      }

      // Use evidence-aware rendering when evaluation data is available
      renderEvaluationWithEvidence(text, lastEvaluationData);
      show(dom.evaluationPanel);
    }

    // â”€â”€â”€ UI Update: Error Display â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    /**
     * Displays an error message to the operator.
     *
     * @param {string} message - Error description
     * @param {boolean} recoverable - Whether the operator can retry
     */
    function showError(message, recoverable) {
      dom.errorMessage.textContent = message;
      dom.errorBanner.className = "error-banner visible " +
        (recoverable ? "recoverable" : "non-recoverable");
    }

    /**
     * Dismisses the error banner.
     */
    function dismissError() {
      hide(dom.errorBanner);
      dom.errorBanner.className = "error-banner";
    }

    // â”€â”€â”€ UI Update: Saved Confirmation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    /**
     * Shows a confirmation that outputs were saved.
     * @param {string[]} paths - File paths that were saved
     */
    function showSavedConfirmation(paths) {
      outputsSaved = true;
      const msg = "Outputs saved: " + paths.join(", ");
      dom.savedMessage.textContent = msg;
      show(dom.savedConfirmation);
      // Hide save button after successful save
      hide(dom.btnSave);
    }

    // â”€â”€â”€ UI Update: Interruption Banner â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    /**
     * Shows the interruption banner after a panic mute.
     */
    function showInterruptionBanner() {
      show(dom.interruptionBanner);
    }

    // â”€â”€â”€ WebSocket Connection â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    /**
     * Establishes a WebSocket connection to the server.
     * Handles connection lifecycle, reconnection display, and message routing.
     */
    function connectWebSocket() {
      if (ws && (ws.readyState === WebSocket.OPEN || ws.readyState === WebSocket.CONNECTING)) {
        return; // Already connected or connecting
      }

      const protocol = window.location.protocol === "https:" ? "wss:" : "ws:";
      const url = protocol + "//" + window.location.host;

      ws = new WebSocket(url);
      audioFormatSent = false;
      // Per Hazard 2: reset pipelineRunId on reconnect to prevent stale filtering
      pipelineRunId = 0;

      ws.binaryType = "arraybuffer"; // For receiving TTS audio

      ws.onopen = function () {
        dom.connectionStatus.textContent = "Connected";
        dom.connectionStatus.style.color = "var(--color-success)";
        // Send audio format handshake immediately on connection
        sendAudioFormatHandshake();
      };

      ws.onmessage = function (event) {
        if (event.data instanceof ArrayBuffer) {
          // Binary message â€” TTS audio chunk
          console.log("[WS] Received binary frame:", event.data.byteLength, "bytes");
          handleTTSAudio(event.data);
          return;
        }
        try {
          const message = JSON.parse(event.data);
          handleServerMessage(message);
        } catch (err) {
          console.error("Failed to parse server message:", err);
        }
      };

      ws.onclose = function () {
        dom.connectionStatus.textContent = "Disconnected";
        dom.connectionStatus.style.color = "var(--color-text-muted)";

        // Fail-safe: if WS drops during playback, deferred state, pending latch,
        // or any non-IDLE state, stop everything. The pendingIdleFromServer check
        // covers the case where the latch is set (e.g., PROCESSINGâ†’IDLE with
        // !ttsPlaying) and the socket drops before handleTTSAudio runs.
        if (ttsPlaying || deferredIdleTransition !== null || pendingIdleFromServer !== null || currentState !== SessionState.IDLE) {
          triggerTTSFailSafe(); // Owns force-stop internally: forceStopTtsAndCancelDeferral() + show written eval + transitionToIdle()
        }

        ws = null;
        audioFormatSent = false;
      };

      ws.onerror = function (err) {
        console.error("WebSocket error:", err);
        dom.connectionStatus.textContent = "Connection Error";
        dom.connectionStatus.style.color = "var(--color-danger)";
      };
    }

    /**
     * Sends a JSON message to the server via WebSocket.
     * @param {Object} message - The message object to send
     */
    function wsSend(message) {
      if (ws && ws.readyState === WebSocket.OPEN) {
        ws.send(JSON.stringify(message));
      } else {
        showError("Not connected to server. Please refresh the page.", false);
      }
    }

    /**
     * Sends the audio_format handshake message.
     * Must be sent before start_recording per the protocol contract.
     */
    function sendAudioFormatHandshake() {
      wsSend({
        type: "audio_format",
        channels: 1,
        sampleRate: 16000,
        encoding: "LINEAR16",
      });
      audioFormatSent = true;
    }

    // â”€â”€â”€ Server Message Handler â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    /**
     * Routes incoming server messages to the appropriate handler.
     * @param {Object} message - Parsed ServerMessage
     */
    function handleServerMessage(message) {
      switch (message.type) {
        case "state_change":
          handleStateChange(message.state);
          break;
        case "transcript_update":
          updateTranscript(message.segments, message.replaceFromIndex);
          break;
        case "elapsed_time":
          updateElapsedTime(message.seconds);
          break;
        case "evaluation_ready":
          lastEvaluationScript = message.script || "";
          lastEvaluationData = message.evaluation || null;
          showEvaluation(message.script);
          break;
        case "tts_complete":
          handleTTSComplete();
          break;
        case "outputs_saved":
          showSavedConfirmation(message.paths);
          break;
        case "error":
          // Fail-safe silent mode: if error occurs during TTS delivery,
          // stop playback and show written evaluation as fallback (Req 7.4)
          if (currentState === SessionState.DELIVERING) {
            triggerTTSFailSafe();
          } else {
            showError(message.message, message.recoverable);
          }
          break;
        case "audio_format_error":
          // Audio format errors are always non-recoverable; stop capture
          if (currentState === SessionState.DELIVERING) {
            triggerTTSFailSafe();
          } else {
            showError("Audio format error: " + message.message, false);
            stopAudioCapture();
          }
          break;
        case "consent_status":
          handleConsentStatus(message.consent);
          break;
        case "duration_estimate":
          handleDurationEstimate(message.estimatedSeconds, message.timeLimitSeconds);
          break;
        case "data_purged":
          handleDataPurged(message.reason);
          break;
        case "pipeline_progress":
          // Ignore stale progress from cancelled pipelines (Hazard 2, Hazard 5)
          if (message.runId < pipelineRunId) break;
          pipelineRunId = message.runId;
          pipelineStage = message.stage;
          updateProcessingIndicator(message.stage);
          updateDeliverButtonState(message.stage);
          break;
        case "vad_speech_end":
          handleVADSpeechEnd(message);
          break;
        case "vad_status":
          handleVADStatus(message);
          break;
        case "video_status":
          // Phase 4: Handle video processing status updates (Req 10.8)
          handleVideoStatus(message);
          break;
        default:
          console.warn("Unknown server message type:", message.type);
      }
    }

    /**
     * Handles state_change messages from the server.
     * Updates UI and manages audio capture lifecycle based on state transitions.
     * @param {string} newState - The new SessionState
     */
    function handleStateChange(newState) {
      const previousState = currentState;

      // Phase 4: Stop video frame capture when leaving RECORDING state (Req 2.1, 2.2)
      // This handles server-initiated transitions (auto-stop, errors) in addition to
      // the explicit stopVideoCapture() calls in onStopSpeech() and onPanicMute().
      if (previousState === SessionState.RECORDING && newState !== SessionState.RECORDING) {
        stopVideoCapture();
      }

      // Echo prevention: hard-stop mic when entering DELIVERING state
      if (newState === SessionState.DELIVERING) {
        hardStopMic();
      }

      // If leaving DELIVERING due to panic mute (going to IDLE without tts_complete),
      // force-stop playback. But if ttsDeliveryComplete is true, let audio play naturally.
      if (previousState === SessionState.DELIVERING && newState !== SessionState.DELIVERING) {
        if (!ttsDeliveryComplete) {
          // Panic mute or error â€” force stop
          console.log("[TTS] Forced stop: leaving DELIVERING without tts_complete");
          forceStopTtsAndCancelDeferral(); // Clears deferral, bumps token, stops audio, clears ttsPlaying
        }
        // If ttsDeliveryComplete is true, audio is playing or finished â€” don't interrupt
      }

      // â”€â”€ IDLE transition decision chain (explicit else-if to prevent reorder mistakes) â”€â”€
      // Branch A: ttsPlaying is true â†’ defer
      if (newState === SessionState.IDLE && ttsPlaying) {
        if (!ttsAudioElement || !ttsAudioElement.src) {
          console.warn("[TTS] ttsPlaying is true but audio element/src missing â€” possible bug");
        }
        // Idempotent: don't overwrite an existing deferral
        if (deferredIdleTransition === null) {
          deferredIdleTransition = { token: playbackInstanceToken };
        }
        // Cooldown will start when deferred IDLE is applied
        return; // Do NOT call updateUI yet
      }

      // Branch B: not playing, non-IDLE state â†’ latch (IDLE arrived before audio)
      // Token-stamped: store current playbackInstanceToken so handleTTSAudio can
      // verify the latch is from the expected session phase (new token = latch + 1).
      // If handleTTSAudio never runs (e.g., server error, no audio sent), the latch
      // is harmless â€” transitionToIdle() clears it, and force-stop paths clear it too.
      else if (newState === SessionState.IDLE && !ttsPlaying && currentState !== SessionState.IDLE) {
        if (pendingIdleFromServer === null) {
          pendingIdleFromServer = { tokenAtLatch: playbackInstanceToken };
        }
        // Don't transition yet â€” wait for handleTTSAudio to consume the latch
        return;
      }

      // Branch C: not playing, already IDLE or no latch needed â†’ immediate transition
      // All non-deferred IDLE transitions go through transitionToIdle() â€” single unified path.
      else if (newState === SessionState.IDLE) {
        transitionToIdle();
        return;
      }

      // Branch D: non-IDLE state â†’ pass through
      updateUI(newState);
    }

    /**
     * Handles incoming TTS audio data (binary WebSocket frames).
     * Creates a Blob URL and plays via HTMLAudioElement for cross-browser
     * compatibility (Safari has issues with Web Audio API decodeAudioData
     * on certain MP3 encodings).
     * @param {ArrayBuffer} audioData - Raw audio bytes from TTS
     */

    /**
     * Primes the TTS audio element during a user gesture so the browser
     * grants playback permission. Called from click handlers before the
     * async WebSocket round-trip that delivers the actual audio data.
     *
     * Two-pronged unlock:
     * 1. Resume a shared AudioContext (unlocks Web Audio for the page).
     * 2. Create the Audio element and call play() with an empty src â€”
     *    even though it fails, the element is now "user-activated" and
     *    subsequent play() calls with real data will succeed.
     */
    function primeTTSAudioElement() {
      cleanupTTSAudio();

      // Unlock the page-level audio policy via AudioContext
      try {
        var ctx = new (window.AudioContext || window.webkitAudioContext)();
        ctx.resume().then(function () {
          ctx.close();
        });
      } catch (e) {
        // AudioContext not available â€” rely on element priming alone
      }

      // Create the element within the user gesture scope
      ttsAudioElement = new Audio();
      ttsAudioElement.preload = "auto";
    }

    function handleTTSAudio(audioData) {
      hasTTSAudio = true;
      ttsDeliveryComplete = false;
      if (!audioData || audioData.byteLength === 0) {
        console.warn("[TTS] Received empty audio data, ignoring");
        return;
      }

      console.log("[TTS] Received audio chunk:", audioData.byteLength, "bytes");

      // Create a Blob URL from the audio data
      if (ttsBlobUrl) {
        URL.revokeObjectURL(ttsBlobUrl);
      }
      const blob = new Blob([audioData], { type: "audio/mpeg" });
      ttsBlobUrl = URL.createObjectURL(blob);

      // Reuse the primed element if available, otherwise create a new one
      if (!ttsAudioElement) {
        ttsAudioElement = new Audio();
        ttsAudioElement.preload = "auto";
      }

      // â”€â”€ MUST-NOT-REFACTOR ORDERING INVARIANT â”€â”€
      // The following steps MUST execute in exactly this order. Reordering any
      // step (especially moving latch consumption after play() or after setting
      // ttsPlaying) can reintroduce the race condition this fix exists to prevent.
      //
      // Explicit ordering: token++ â†’ capture â†’ consume latch â†’ set handlers â†’ ttsPlaying=true â†’ play()

      // 1. Increment token for this new playback instance
      playbackInstanceToken++;
      // 2. Capture token for closure
      const currentToken = playbackInstanceToken;

      // 3. Consume the pendingIdleFromServer latch: if IDLE arrived before this
      // function ran (ordering edge case), create the deferral now, bound to the
      // new token. Token validation: only consume if the latch token matches
      // expected progression (latchToken + 1 === currentToken). This prevents a
      // stale latch from an earlier session phase from "attaching" to unrelated
      // audio (e.g., replay triggered much later after other state changes).
      // If the token doesn't match, the latch is stale â€” discard it.
      if (pendingIdleFromServer !== null) {
        var latchIsRelevant = (pendingIdleFromServer.tokenAtLatch + 1 === currentToken);
        pendingIdleFromServer = null; // Always clear â€” consumed or discarded
        if (latchIsRelevant && deferredIdleTransition === null) {
          deferredIdleTransition = { token: currentToken };
        }
      }

      // 3b. Adopt existing deferral: if a deferredIdleTransition was created for
      // a previous playback (e.g., IDLE arrived during playback N, creating deferral
      // with token N, then new audio arrives bumping token to N+1), update the
      // deferral token to the current playback. Without this, the new onended
      // (token N+1) would never match the old deferral (token N), leaving a dead
      // deferral that prevents IDLE transition.
      if (deferredIdleTransition !== null && deferredIdleTransition.token !== currentToken) {
        deferredIdleTransition = { token: currentToken };
      }

      // 4. Set handlers BEFORE play() so they're ready for immediate events
      ttsAudioElement.onplay = function () {
        console.log("[TTS] Audio playback started");
      };

      ttsAudioElement.onended = function () {
        console.log("[TTS] Audio playback ended naturally");
        // Clean up state FIRST so updateUI sees consistent flags
        ttsPlaying = false;
        cleanupTTSAudio();
        // Then apply deferred IDLE (which calls updateUI via transitionToIdle)
        applyDeferredIdle(currentToken);
      };

      ttsAudioElement.onerror = function (e) {
        console.error("[TTS] Audio element error:", ttsAudioElement ? ttsAudioElement.error : e);
        // onerror: triggerTTSFailSafe owns the force-stop internally
        triggerTTSFailSafe();
      };

      // 5. Set ttsPlaying=true immediately before play() â€” covers the decode gap
      ttsPlaying = true;

      // 6. Swap in the real audio source and play
      ttsAudioElement.src = ttsBlobUrl;
      ttsAudioElement.play().then(function () {
        console.log("[TTS] play() promise resolved, audio is playing");
      }).catch(function (err) {
        console.error("[TTS] play() promise rejected:", err);
        // play() can reject without firing onerror â€” triggerTTSFailSafe owns the force-stop
        triggerTTSFailSafe();
      });
    }

    /**
     * Cleans up TTS audio resources (element and Blob URL).
     */
    function cleanupTTSAudio() {
      if (ttsAudioElement) {
        ttsAudioElement.onplay = null;
        ttsAudioElement.onended = null;
        ttsAudioElement.onerror = null;
        ttsAudioElement.pause();
        ttsAudioElement.src = "";
        ttsAudioElement = null;
      }
      if (ttsBlobUrl) {
        URL.revokeObjectURL(ttsBlobUrl);
        ttsBlobUrl = null;
      }
    }

    /**
     * Stops all TTS audio playback and cleans up resources.
     * Nulls out event handlers BEFORE pausing to prevent lingering handlers
     * from firing on reused elements and to reduce reentrancy risk.
     */
    function stopTTSPlayback() {
      // Null out event handlers first â€” prevents onended/onerror from firing
      // during pause/cleanup, which could cause reentrancy issues
      if (ttsAudioElement) {
        ttsAudioElement.onended = null;
        ttsAudioElement.onerror = null;
        ttsAudioElement.onplay = null;
      }
      cleanupTTSAudio();
      ttsPlaying = false;
      ttsDeliveryComplete = false;
    }

    /**
     * Applies a deferred IDLE transition if the token matches.
     * Called from onended handler after ttsPlaying and cleanup are done.
     *
     * No-op cases (both are safe and expected):
     * - deferredIdleTransition is null (no deferral was pending â€” e.g., audio ended
     *   before state_change: idle arrived, or deferral was already cancelled)
     * - Token mismatch (stale onended from a previous/cancelled playback)
     *
     * Clears deferredIdleTransition BEFORE calling transitionToIdle() â€” this ordering
     * prevents reentrancy (e.g., duplicate onended) from re-running transitionToIdle().
     *
     * @param {number} token - The playback instance token from the onended closure
     */
    function applyDeferredIdle(token) {
      if (deferredIdleTransition === null) return; // No deferral pending â€” no-op
      if (deferredIdleTransition.token !== token) return; // Stale event â€” ignore

      deferredIdleTransition = null;
      transitionToIdle();
    }

    /**
     * Single authoritative function for transitioning to IDLE with cooldown.
     * ALL code paths that transition to IDLE must go through this function â€”
     * including triggerTTSFailSafe(), applyDeferredIdle(), and handleStateChange().
     * This prevents double-cooldown, missing-cooldown, and micro-hiccup bugs.
     *
     * Order: clear pendingIdleFromServer first, then updateUI (pure visual state flip),
     * then startCooldown (timers/side effects). This prevents micro-hiccups from
     * cooldown DOM/timer work before the UI state is consistent.
     *
     * Idempotency guard: currentState === SessionState.IDLE && cooldownTimer !== null
     * â€” precise enough to distinguish "IDLE with cooldown active" (no-op) from
     * "IDLE without cooldown" (needs cooldown start, e.g., recovery paths).
     * Using just currentState === IDLE would skip cooldown in legitimate cases.
     */
    function transitionToIdle() {
      // Precise idempotency: skip only if already IDLE AND cooldown is running.
      // If IDLE but no cooldown (recovery path), we still need to start cooldown.
      if (currentState === SessionState.IDLE && cooldownTimer !== null) return;
      pendingIdleFromServer = null; // Clear latch â€” we're applying IDLE now
      updateUI(SessionState.IDLE);
      startCooldown();
    }

    /**
     * Cancels any pending deferred IDLE transition and invalidates the current
     * playback token so that late onended events from a previous playback
     * cannot apply a future deferral.
     *
     * DOES NOT clear ttsPlaying or stop playback â€” that is the responsibility
     * of stopTTSPlayback(). DOES NOT clear pendingIdleFromServer â€” that is
     * cleared by forceStopTtsAndCancelDeferral() or transitionToIdle().
     * Use forceStopTtsAndCancelDeferral() for force-stop paths.
     *
     * Called internally by forceStopTtsAndCancelDeferral().
     */
    function cancelDeferredIdle() {
      deferredIdleTransition = null;
      // Monotonic bump: any in-flight onended closure holding the old token
      // will fail the token match in applyDeferredIdle()
      playbackInstanceToken++;
    }

    /**
     * Composite force-stop: cancels deferral, bumps token, clears latch, stops audio,
     * clears ttsPlaying.
     *
     * Post-conditions hold immediately after the call:
     * - deferredIdleTransition === null
     * - pendingIdleFromServer === null
     * - playbackInstanceToken bumped (late onended events are harmless due to token
     *   invalidation + handler nulling in stopTTSPlayback)
     * - ttsPlaying === false
     * - audio element event handlers nulled
     *
     * Note: this is NOT truly atomic against queued browser events â€” a late onended
     * CAN still fire after this call returns. That is safe because the token bump +
     * handler nulling ensures late callbacks are no-ops. Do not assume "no late
     * callbacks possible"; assume "late callbacks are harmless."
     *
     * ALL force-stop paths (panic mute, revoke consent, ws close, onerror, play reject)
     * MUST call this instead of calling cancelDeferredIdle() and stopTTSPlayback() separately.
     */
    function forceStopTtsAndCancelDeferral() {
      cancelDeferredIdle();          // Clear deferral + bump token
      pendingIdleFromServer = null;  // Clear latch
      stopTTSPlayback();             // Stop audio + clear ttsPlaying + null handlers
    }

    /**
     * Fail-safe silent mode: stops TTS playback, shows written evaluation
     * as fallback, and transitions UI to IDLE. No automatic retry.
     * Per meeting-safety-controls.md: if any critical error occurs during
     * TTS delivery, playback stops immediately and the written evaluation
     * is displayed as fallback.
     *
     * Contract: calls forceStopTtsAndCancelDeferral() internally (idempotent if
     * already stopped), shows written evaluation panel, then calls transitionToIdle().
     * Callers never need to force-stop before calling this â€” single entry point,
     * no double-stop/double-cooldown risk.
     */
    function triggerTTSFailSafe() {
      forceStopTtsAndCancelDeferral();

      // Show the written evaluation as fallback (Requirement 7.4)
      if (lastEvaluationScript) {
        showEvaluation(lastEvaluationScript);
      }
      show(dom.evaluationPanel);

      // Show a non-recoverable error explaining the fallback
      showError("Audio playback failed. The written evaluation is displayed below.", false);

      // Transition UI to IDLE via single authoritative path
      transitionToIdle();
    }

    /**
     * Handles tts_complete message â€” TTS delivery finished on the server side.
     * Does NOT stop playback â€” the audio may still be playing.
     * Sets a flag so the client knows no more chunks are coming.
     */
    function handleTTSComplete() {
      console.log("[TTS] Server signaled tts_complete, audio may still be playing");
      ttsDeliveryComplete = true;
    }

    // â”€â”€â”€ Phase 2: Consent, Duration, and Purge Handlers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    /**
     * Handles consent_status message from the server.
     * Updates local consent state and UI.
     * @param {Object|null} consent - ConsentRecord or null
     */
    function handleConsentStatus(consent) {
      if (consent) {
        consentSpeakerName = consent.speakerName || "";
        consentConfirmed = consent.consentConfirmed || false;
        // Sync form inputs with server state
        dom.speakerNameInput.value = consentSpeakerName;
        dom.consentCheckbox.checked = consentConfirmed;
      } else {
        consentSpeakerName = "";
        consentConfirmed = false;
        dom.speakerNameInput.value = "";
        dom.consentCheckbox.checked = false;
      }
      updateUI(currentState);
    }

    /**
     * Handles duration_estimate message from the server.
     * Updates the duration estimate display.
     * @param {number} estimatedSeconds - Estimated evaluation duration
     * @param {number} timeLimitSeconds - Configured time limit
     */
    function handleDurationEstimate(estimatedSeconds, timeLimitSeconds) {
      estimatedDuration = estimatedSeconds;
      configuredTimeLimit = timeLimitSeconds;
      dom.timeLimitInput.value = timeLimitSeconds;
      updateDurationEstimateDisplay(estimatedSeconds, timeLimitSeconds);
      // Show the estimate if we're in PROCESSING state
      if (currentState === SessionState.PROCESSING) {
        show(dom.durationEstimate);
      }
    }

    /**
     * Handles data_purged message from the server.
     * Clears all displayed data and updates UI based on purge reason.
     * @param {string} reason - "opt_out" or "auto_purge"
     */
    function handleDataPurged(reason) {
      // Clear all displayed data
      segments = [];
      renderTranscript();
      dom.evaluationContent.innerHTML =
        '<div class="evaluation-empty">Evaluation will appear here after delivery...</div>';
      hide(dom.evaluationPanel);
      hide(dom.transcriptPanel);
      hasEvaluationData = false;
      hasTTSAudio = false;
      lastEvaluationScript = "";
      lastEvaluationData = null;
      lastVideoQualityGrade = null;
      estimatedDuration = null;
      hide(dom.durationEstimate);
      forceStopTtsAndCancelDeferral();
      // Reset pipeline state on data purge (Req 6.3)
      pipelineStage = "idle";
      pipelineRunId = 0;
      // Clear evidence highlights (Phase 3)
      clearEvidenceHighlight();
      // Hide video quality grade badge
      if (videoDom.qualityGrade) {
        videoDom.qualityGrade.className = "video-quality-grade";
      }

      if (reason === "opt_out") {
        // Permanent purge â€” disable Save Outputs for this session
        dataPurged = true;
        // Reset consent form
        consentSpeakerName = "";
        consentConfirmed = false;
        dom.speakerNameInput.value = "";
        dom.consentCheckbox.checked = false;
        // Reset video consent (Phase 4, Req 1.7)
        videoConsentEnabled = false;
        videoStreamReady = false;
        dom.videoConsentCheckbox.checked = false;
        disable(dom.videoConsentCheckbox);
        hideVideoConsentError();
        releaseCamera();
        // Reset video FPS config to default
        videoFpsConfig = 2;
        dom.videoFpsSlider.value = 2;
        dom.videoFpsValue.textContent = "2 FPS";
        // Reset VAD config to defaults (Phase 3, Req 3.1, 3.5)
        vadEnabled = true;
        vadSilenceThreshold = 5;
        dom.vadEnabledCheckbox.checked = true;
        dom.vadThresholdSlider.value = 5;
        dom.vadThresholdValue.textContent = "5s";
        resetVadEnergyState();
        // Reset project context form (Phase 3, Req 4.1)
        resetProjectContextForm();
        // Show opt-out purge banner
        dom.purgeMessage.textContent = "Speaker data has been purged per opt-out request.";
        dom.purgeBanner.className = "purge-banner visible opt-out";
      } else if (reason === "auto_purge") {
        // Auto-purge â€” informational
        // Reset video consent (Phase 4)
        videoConsentEnabled = false;
        videoStreamReady = false;
        dom.videoConsentCheckbox.checked = false;
        disable(dom.videoConsentCheckbox);
        hideVideoConsentError();
        releaseCamera();
        // Reset video FPS config to default
        videoFpsConfig = 2;
        dom.videoFpsSlider.value = 2;
        dom.videoFpsValue.textContent = "2 FPS";
        // Reset VAD config to defaults (Phase 3)
        vadEnabled = true;
        vadSilenceThreshold = 5;
        dom.vadEnabledCheckbox.checked = true;
        dom.vadThresholdSlider.value = 5;
        dom.vadThresholdValue.textContent = "5s";
        resetVadEnergyState();
        // Reset project context form (Phase 3)
        resetProjectContextForm();
        dom.purgeMessage.textContent = "Session data auto-purged after timeout.";
        dom.purgeBanner.className = "purge-banner visible auto-purge";
      }

      // Transition to IDLE via single authoritative path
      transitionToIdle();
    }

    // â”€â”€â”€ Audio Capture: Mic + AudioWorklet â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    /**
     * Requests microphone permission and checks for available audio input devices.
     * @returns {Promise<boolean>} true if mic is available and permission granted
     */
    async function checkMicPermission() {
      try {
        // Check for available audio input devices
        const devices = await navigator.mediaDevices.enumerateDevices();
        const audioInputs = devices.filter(function (d) { return d.kind === "audioinput"; });
        if (audioInputs.length === 0) {
          showError("No microphone detected. Please connect a microphone and refresh.", false);
          disable(dom.btnStart);
          return false;
        }
        return true;
      } catch (err) {
        showError("Cannot access media devices: " + err.message, false);
        disable(dom.btnStart);
        return false;
      }
    }

    /**
     * Starts audio capture: requests mic, creates AudioContext + AudioWorklet,
     * and begins streaming audio chunks to the server via WebSocket.
     * @returns {Promise<boolean>} true if audio capture started successfully
     */
    async function startAudioCapture() {
      try {
        // Request mic permission
        mediaStream = await navigator.mediaDevices.getUserMedia({ audio: true });

        // Create AudioContext (or reuse if already created)
        if (!audioContext || audioContext.state === "closed") {
          audioContext = new AudioContext();
        }
        // Resume if suspended (browsers require user gesture)
        if (audioContext.state === "suspended") {
          await audioContext.resume();
        }

        // Load the AudioWorklet processor module
        await audioContext.audioWorklet.addModule("audio-worklet.js");

        // Create source node from mic stream
        sourceNode = audioContext.createMediaStreamSource(mediaStream);

        // Create AudioWorklet node
        workletNode = new AudioWorkletNode(audioContext, "audio-capture-processor");

        // Listen for audio chunks from the worklet
        workletNode.port.onmessage = function (event) {
          if (event.data && event.data.type === "audio_chunk") {
            // Check if VAD energy has gone stale and we need to fall back (Req 10.4)
            checkVadEnergyFallback();
            // Update audio level meter (skipped if VAD energy is active, Req 10.3)
            if (typeof event.data.level === "number") {
              updateAudioLevel(event.data.level);
            }
            // Send audio chunk as binary WebSocket frame
            if (ws && ws.readyState === WebSocket.OPEN && currentState === SessionState.RECORDING) {
              ws.send(event.data.samples);
            }
          }
        };

        // Connect the pipeline: mic â†’ worklet (worklet doesn't output to speakers)
        sourceNode.connect(workletNode);
        // Connect worklet to destination to keep the audio graph alive
        // (AudioWorklet needs to be connected to process)
        workletNode.connect(audioContext.destination);

        return true;
      } catch (err) {
        if (err.name === "NotAllowedError" || err.name === "PermissionDeniedError") {
          showError("Microphone permission denied. Please allow microphone access and try again.", false);
        } else if (err.name === "NotFoundError") {
          showError("No microphone found. Please connect a microphone and try again.", false);
        } else {
          showError("Failed to start audio capture: " + err.message, true);
        }
        disable(dom.btnStart);
        return false;
      }
    }

    /**
     * Stops the AudioWorklet and disconnects the audio graph.
     * Keeps the MediaStream alive for potential restart.
     */
    function stopAudioCapture() {
      if (workletNode) {
        // Tell the worklet processor to stop
        workletNode.port.postMessage({ type: "stop" });
        workletNode.disconnect();
        workletNode = null;
      }
      if (sourceNode) {
        sourceNode.disconnect();
        sourceNode = null;
      }
    }

    /**
     * Hard-stops the MediaStream tracks immediately.
     * Used for panic mute and echo prevention during DELIVERING state.
     * After this, a new getUserMedia call is needed to re-arm the mic.
     */
    function hardStopMic() {
      // Stop the AudioWorklet first
      stopAudioCapture();

      // Phase 4: Stop video frame capture (echo prevention â€” no video during delivery)
      stopVideoCapture();

      // Hard-stop all MediaStream tracks (not just mute â€” fully release the mic)
      if (mediaStream) {
        mediaStream.getTracks().forEach(function (track) {
          track.stop();
        });
        mediaStream = null;
      }
    }

    // â”€â”€â”€ Cooldown Logic â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    // After TTS completes, wait 2-3 seconds before allowing mic re-arm.
    // This prevents the system from capturing its own TTS echo.

    /**
     * Starts the post-TTS cooldown period.
     * During cooldown, the "Start Speech" button is disabled.
     */
    function startCooldown() {
      inCooldown = true;
      disable(dom.btnStart);
      dom.statusText.textContent = "Cooldown â€” mic re-arming shortly...";

      cooldownTimer = setTimeout(function () {
        inCooldown = false;
        cooldownTimer = null;
        // Re-enable Start Speech only if consent is confirmed
        if (consentConfirmed && consentSpeakerName.trim().length > 0) {
          enable(dom.btnStart);
        }
        if (hasTTSAudio && hasEvaluationData && !dataPurged) {
          enable(dom.btnReplay);
        }
        dom.statusText.textContent = STATUS_TEXT[SessionState.IDLE];
      }, COOLDOWN_MS);
    }

    /**
     * Cancels any active cooldown (e.g., on panic mute during cooldown).
     */
    function clearCooldown() {
      if (cooldownTimer) {
        clearTimeout(cooldownTimer);
        cooldownTimer = null;
      }
      inCooldown = false;
    }

    // â”€â”€â”€ Button Click Handlers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    async function onStartSpeech() {
      // Guard: don't start during cooldown
      if (inCooldown) return;

      // Guard: consent must be confirmed (Req 2.3)
      if (!consentConfirmed || consentSpeakerName.trim().length === 0) {
        showError("Please enter the speaker's name and confirm consent before starting.", true);
        return;
      }

      // Guard: need WebSocket connection
      if (!ws || ws.readyState !== WebSocket.OPEN) {
        showError("Not connected to server. Please wait for connection or refresh.", false);
        return;
      }

      // Reset state for new recording
      segments = [];
      hasEvaluationData = false;
      hasTTSAudio = false;
      outputsSaved = false;
      dataPurged = false;
      estimatedDuration = null;
      lastEvaluationScript = "";
      lastEvaluationData = null;
      // Reset pipeline state for new recording (Req 2.5, 6.3)
      pipelineStage = "idle";
      pipelineRunId = 0;
      forceStopTtsAndCancelDeferral();
      dismissError();
      hideVideoConsentError();
      clearEvidenceHighlight();
      hide(dom.interruptionBanner);
      hide(dom.savedConfirmation);
      hide(dom.purgeBanner);
      hide(dom.durationEstimate);
      dom.evaluationContent.innerHTML =
        '<div class="evaluation-empty">Evaluation will appear here after delivery...</div>';
      hide(dom.evaluationPanel);

      // Ensure audio format handshake was sent
      if (!audioFormatSent) {
        sendAudioFormatHandshake();
      }

      // Start audio capture (mic + AudioWorklet)
      const captureStarted = await startAudioCapture();
      if (!captureStarted) {
        return; // Error already shown by startAudioCapture
      }

      // Phase 4: Camera is already acquired via video consent toggle (Req 1.5)
      // Only start video capture if video consent is enabled and camera is ready

      // Send start_recording command to server
      wsSend({ type: "start_recording" });

      if (videoConsentEnabled && videoStream) {
        startVideoCapture();
      }

      // Optimistic UI update (server will confirm via state_change)
      updateUI(SessionState.RECORDING);
      updateElapsedTime(0);
    }

    function onStopSpeech() {
      // Stop the AudioWorklet but keep MediaStream alive for potential restart
      stopAudioCapture();

      // Phase 4: Stop video frame capture (camera stays alive for potential restart)
      stopVideoCapture();

      // Send stop_recording command to server
      wsSend({ type: "stop_recording" });

      // Optimistic UI update
      updateUI(SessionState.PROCESSING);
    }

    async function onDeliverEvaluation() {
      // Echo prevention: hard-stop mic tracks before TTS delivery
      hardStopMic();

      // Prime the audio element during the user gesture so the browser
      // grants playback permission. The real TTS source is swapped in
      // when the WebSocket delivers the audio data.
      primeTTSAudioElement();

      // Send deliver_evaluation command to server
      wsSend({ type: "deliver_evaluation" });

      // Optimistic UI update
      updateUI(SessionState.DELIVERING);
    }

    async function onReplayEvaluation() {
      // Guard: don't replay during cooldown
      if (inCooldown) return;

      // Echo prevention: hard-stop mic before replay (same as initial delivery)
      hardStopMic();

      // Prime the audio element during the user gesture (same as deliver)
      primeTTSAudioElement();

      // Send replay_tts command to server
      wsSend({ type: "replay_tts" });

      // Optimistic UI update to DELIVERING state
      updateUI(SessionState.DELIVERING);
    }

    function onSaveOutputs() {
      // Guard: don't save if data was purged
      if (dataPurged) return;

      // Send save_outputs command to server
      wsSend({ type: "save_outputs" });
      disable(dom.btnSave);
    }

    function onRevokeConsent() {
      // Confirmation dialog per privacy-and-retention.md and meeting-safety-controls.md
      const confirmed = window.confirm(
        "This will permanently delete all data from this speech. Continue?"
      );
      if (!confirmed) return;

      // Hard-stop mic if recording
      hardStopMic();
      // Phase 4: Release camera on consent revocation
      releaseCamera();
      // Atomic: cancel deferral + bump token + stop audio + clear ttsPlaying + clear latch
      forceStopTtsAndCancelDeferral();
      clearCooldown();

      // Send revoke_consent command to server
      wsSend({ type: "revoke_consent" });

      // The server will respond with data_purged message which handles the rest
    }

    function onPanicMute() {
      // Panic mute: immediate action, no confirmation dialog
      // Hard-stop mic immediately
      hardStopMic();
      // Phase 4: Stop video capture immediately
      stopVideoCapture();
      // Atomic: cancel deferral + bump token + stop audio + clear ttsPlaying + clear latch
      forceStopTtsAndCancelDeferral();
      clearCooldown();
      // Reset pipeline state on panic mute (Req 6.3)
      pipelineStage = "idle";
      pipelineRunId = 0;

      // Send panic_mute command to server
      wsSend({ type: "panic_mute" });

      // Optimistic UI update
      updateUI(SessionState.IDLE);
      showInterruptionBanner();
    }

    // â”€â”€â”€ Utility Functions â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    function show(el) {
      el.classList.add("visible");
      el.classList.remove("hidden");
    }

    function hide(el) {
      el.classList.remove("visible");
      el.classList.add("hidden");
    }

    function enable(el) {
      el.disabled = false;
    }

    function disable(el) {
      el.disabled = true;
    }

    function showElapsedTime() {
      dom.elapsedTime.classList.add("visible");
    }

    function hideElapsedTime() {
      dom.elapsedTime.classList.remove("visible");
    }

    /**
     * Formats seconds into MM:SS timestamp string.
     * @param {number} totalSeconds
     * @returns {string}
     */
    function formatTimestamp(totalSeconds) {
      const mins = Math.floor(totalSeconds / 60);
      const secs = Math.floor(totalSeconds % 60);
      return String(mins).padStart(2, "0") + ":" + String(secs).padStart(2, "0");
    }

    /**
     * Escapes HTML special characters to prevent XSS.
     * @param {string} text
     * @returns {string}
     */
    function escapeHtml(text) {
      const div = document.createElement("div");
      div.textContent = text;
      return div.innerHTML;
    }

    // â”€â”€â”€ Phase 4: Video Frame Streaming State (Req 2.1, 2.2, 16.1) â”€â”€
    /** @type {MediaStream|null} Camera video stream */
    let videoStream = null;
    /** @type {number|null} Interval ID for frame capture at 5 FPS */
    let videoCaptureInterval = null;
    /** Frame sequence number, reset per recording session */
    let videoFrameSeq = 0;
    /** performance.now() at recording start â€” shared time base with audio */
    let recordingStartPerfNow = 0;
    /** Total frames sent this session (for stats display) */
    let videoFramesSent = 0;
    /** Total frames skipped due to backpressure this session */
    let videoFramesSkipped = 0;
    /** Backpressure threshold: 2 MB */
    const VIDEO_BACKPRESSURE_BYTES = 2 * 1024 * 1024;
    /** Frame capture interval: 200ms = 5 FPS client cap */
    const VIDEO_CAPTURE_INTERVAL_MS = 200;
    /** JPEG quality for frame capture (0.0 - 1.0) */
    const VIDEO_JPEG_QUALITY = 0.7;

    // â”€â”€â”€ Phase 4: Video DOM References â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    const videoDom = {
      previewContainer: document.getElementById("video-preview-container"),
      preview: document.getElementById("video-preview"),
      captureCanvas: document.getElementById("video-capture-canvas"),
      frameStats: document.getElementById("video-frame-stats"),
      qualityGrade: document.getElementById("video-quality-grade"),
    };

    /** Last video quality grade from final video_status message */
    let lastVideoQualityGrade = null;

    /**
     * Encodes a video frame into the TM-prefixed wire format for browser.
     * Format: [0x54 0x4D][0x56][uint24 header len][header JSON UTF-8][JPEG bytes]
     *
     * Uses ArrayBuffer/DataView/Uint8Array for browser compatibility (no Node.js Buffer).
     *
     * @param {Object} header - Frame header with timestamp, seq, width, height
     * @param {Uint8Array} jpegBytes - JPEG image data
     * @returns {ArrayBuffer} Encoded binary frame
     */
    function encodeVideoFrameBrowser(header, jpegBytes) {
      var headerJson = JSON.stringify(header);
      var encoder = new TextEncoder();
      var headerBytes = encoder.encode(headerJson);
      var headerLen = headerBytes.length;

      // Total: 2 (magic) + 1 (type) + 3 (header len) + headerLen + jpegBytes.length
      var totalLen = 6 + headerLen + jpegBytes.length;
      var buf = new ArrayBuffer(totalLen);
      var view = new DataView(buf);
      var arr = new Uint8Array(buf);

      // TM magic prefix
      view.setUint8(0, 0x54); // 'T'
      view.setUint8(1, 0x4D); // 'M'
      // Type byte: video
      view.setUint8(2, 0x56); // 'V'
      // uint24 big-endian header length
      view.setUint8(3, (headerLen >> 16) & 0xFF);
      view.setUint8(4, (headerLen >> 8) & 0xFF);
      view.setUint8(5, headerLen & 0xFF);
      // Header JSON bytes
      arr.set(headerBytes, 6);
      // JPEG payload
      arr.set(jpegBytes, 6 + headerLen);

      return buf;
    }

    /**
     * Acquires camera access via getUserMedia and sets up the video preview.
     * @returns {Promise<boolean>} true if camera acquired successfully
     */
    async function acquireCamera() {
      try {
        videoStream = await navigator.mediaDevices.getUserMedia({
          video: { width: { ideal: 640 }, height: { ideal: 480 }, frameRate: { ideal: 10 } }
        });
        videoDom.preview.srcObject = videoStream;
        show(videoDom.previewContainer);
        return true;
      } catch (err) {
        console.warn("[Video] Camera acquisition failed:", err.message);
        videoStream = null;
        return false;
      }
    }

    /**
     * Starts video frame capture at 5 FPS client cap.
     * Captures JPEG frames from the video element via canvas,
     * encodes them in TM-prefixed wire format, and sends over WebSocket.
     */
    function startVideoCapture() {
      if (!videoStream || !videoDom.preview.videoWidth) {
        console.warn("[Video] Cannot start capture â€” no video stream or video not ready");
        return;
      }

      // Reset per-session counters
      videoFrameSeq = 0;
      videoFramesSent = 0;
      videoFramesSkipped = 0;
      lastVideoQualityGrade = null;
      recordingStartPerfNow = performance.now();

      // Hide video quality grade from previous session
      if (videoDom.qualityGrade) {
        videoDom.qualityGrade.className = "video-quality-grade";
      }

      var canvas = videoDom.captureCanvas;
      var ctx = canvas.getContext("2d");

      videoCaptureInterval = setInterval(function () {
        // Guard: only send during RECORDING state with video consent (Req 2.1, 10.4)
        if (currentState !== SessionState.RECORDING) return;
        if (!videoConsentEnabled) return;
        // Guard: need open WebSocket
        if (!ws || ws.readyState !== WebSocket.OPEN) return;
        // Guard: video element must have dimensions
        if (!videoDom.preview.videoWidth || !videoDom.preview.videoHeight) return;

        // Backpressure guard: skip frame if bufferedAmount > 2MB
        if (ws.bufferedAmount > VIDEO_BACKPRESSURE_BYTES) {
          videoFramesSkipped++;
          updateVideoFrameStats();
          return;
        }

        var vw = videoDom.preview.videoWidth;
        var vh = videoDom.preview.videoHeight;

        // Set canvas to match video dimensions
        canvas.width = vw;
        canvas.height = vh;

        // Draw current video frame to canvas
        ctx.drawImage(videoDom.preview, 0, 0, vw, vh);

        // Convert to JPEG blob via toBlob for async, non-blocking encoding
        canvas.toBlob(function (blob) {
          if (!blob) return;
          // Guard again â€” state or consent may have changed during async toBlob
          if (currentState !== SessionState.RECORDING) return;
          if (!videoConsentEnabled) return;
          if (!ws || ws.readyState !== WebSocket.OPEN) return;
          if (ws.bufferedAmount > VIDEO_BACKPRESSURE_BYTES) {
            videoFramesSkipped++;
            updateVideoFrameStats();
            return;
          }

          // Read blob as ArrayBuffer
          var reader = new FileReader();
          reader.onload = function () {
            // Final guard after async read â€” verify state and consent
            if (currentState !== SessionState.RECORDING) return;
            if (!videoConsentEnabled) return;
            if (!ws || ws.readyState !== WebSocket.OPEN) return;

            var jpegBytes = new Uint8Array(reader.result);

            // Build header with timestamp relative to recording start
            var header = {
              timestamp: (performance.now() - recordingStartPerfNow) / 1000,
              seq: videoFrameSeq++,
              width: vw,
              height: vh
            };

            // Encode in TM-prefixed wire format and send as binary
            var encoded = encodeVideoFrameBrowser(header, jpegBytes);
            ws.send(encoded);

            videoFramesSent++;
            updateVideoFrameStats();
          };
          reader.readAsArrayBuffer(blob);
        }, "image/jpeg", VIDEO_JPEG_QUALITY);
      }, VIDEO_CAPTURE_INTERVAL_MS);
    }

    /**
     * Stops video frame capture.
     */
    function stopVideoCapture() {
      if (videoCaptureInterval !== null) {
        clearInterval(videoCaptureInterval);
        videoCaptureInterval = null;
      }
    }

    /**
     * Releases the camera stream and hides the preview.
     */
    function releaseCamera() {
      stopVideoCapture();
      if (videoStream) {
        videoStream.getTracks().forEach(function (track) { track.stop(); });
        videoStream = null;
      }
      if (videoDom.preview) {
        videoDom.preview.srcObject = null;
      }
      hide(videoDom.previewContainer);
    }

    /**
     * Updates the video frame stats display.
     */
    function updateVideoFrameStats() {
      if (videoDom.frameStats) {
        videoDom.frameStats.textContent = "Sent: " + videoFramesSent + " / Skipped: " + videoFramesSkipped;
      }
    }

    /**
     * Handles video_status messages from the server.
     * Updates the video frame stats display during recording. (Req 10.8, 17.1)
     * Shows frames processed/dropped, latency warning if >500ms,
     * and captures video quality grade from final status after stop_recording.
     * @param {Object} message - The video_status message
     */
    function handleVideoStatus(message) {
      // Capture video quality grade from final status (present after stop_recording)
      if (message.videoQualityGrade) {
        lastVideoQualityGrade = message.videoQualityGrade;
      }

      // Show server-side frame stats during recording
      if (videoDom.frameStats && currentState === SessionState.RECORDING) {
        const processed = message.framesProcessed || 0;
        const dropped = message.framesDropped || 0;
        const latency = message.processingLatencyMs || 0;
        let text = "Processed: " + processed + " / Dropped: " + dropped;
        if (latency > 500) {
          text += " âš ï¸";
        }
        videoDom.frameStats.textContent = text;
        if (latency > 500) {
          videoDom.frameStats.title = "Processing latency: " + Math.round(latency) + "ms (high)";
        } else {
          videoDom.frameStats.title = "";
        }
      }

      // Show video quality grade after evaluation (final status message)
      if (message.videoQualityGrade) {
        showVideoQualityGrade(message.videoQualityGrade);
      }
    }

    /**
     * Displays the video quality grade badge in the evaluation panel header.
     * @param {"good"|"degraded"|"poor"} grade
     */
    function showVideoQualityGrade(grade) {
      if (!videoDom.qualityGrade) return;
      videoDom.qualityGrade.className = "video-quality-grade visible " + grade;
      var labels = { good: "Video: Good", degraded: "Video: Degraded", poor: "Video: Poor" };
      videoDom.qualityGrade.textContent = labels[grade] || "Video: " + grade;
    }

    // â”€â”€â”€ Initialize â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    // Set initial UI state and establish WebSocket connection
    updateUI(SessionState.IDLE);

    // Connect WebSocket on page load
    connectWebSocket();

    // Check mic availability on load (non-blocking)
    checkMicPermission();

    // â”€â”€â”€ Phase 2: Consent & Time Limit Event Listeners â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    // Listen for consent form changes
    dom.speakerNameInput.addEventListener("input", onConsentChange);
    dom.consentCheckbox.addEventListener("change", onConsentChange);

    // Listen for time limit changes
    dom.timeLimitInput.addEventListener("change", onTimeLimitChange);

    // Phase 3: Listen for VAD config changes
    dom.vadEnabledCheckbox.addEventListener("change", onVADConfigChange);
    dom.vadThresholdSlider.addEventListener("change", onVADConfigChange);
    dom.vadThresholdSlider.addEventListener("input", onVADThresholdInput);

    // Phase 3: Listen for project context form changes
    dom.speechTitleInput.addEventListener("input", onSpeechTitleChange);
    dom.projectTypeSelect.addEventListener("change", onProjectTypeChange);
    dom.objectivesTextarea.addEventListener("input", onObjectivesChange);

    // Phase 4: Listen for video consent and FPS config changes
    dom.videoConsentCheckbox.addEventListener("change", onVideoConsentChange);
    dom.videoFpsSlider.addEventListener("change", onVideoFpsChange);
    dom.videoFpsSlider.addEventListener("input", onVideoFpsInput);
  </script>
</body>
</html>
